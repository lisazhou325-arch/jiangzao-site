大脑不是在做"预测下一个词"，它在执行数亿年进化塑造出的极度具体的代价函数——这才是AI真正缺失的东西。

我们向大模型投喂了远超人类一生的数据，它们却连常识推理都做不可靠。问题不在算力，在于我们根本不理解大脑在算什么。

多巴胺不只是奖励信号。它同时编码预测误差、控制学习时机、调节注意力——我们连它的计算模型都没有。

AI不睡觉。而睡眠可能正是大脑巩固记忆、修剪突触的关键离线阶段——这个缺口，当前范式完全没有答案。

人类基因组计划花了30亿美元，改变了整个生物学。神经科学需要同等量级的投入，否则我们只是在黑暗中摸索。

---

📝 创作说明
嘉宾：Adam Marblestone
选题方向：神经科学家视角下AI的根本性缺口——代价函数、睡眠与具身性
内容评分：信息密度 8/10，数据支撑 6/10，洞见深度 9/10
目标字数：2100 字
核心价值：从神经科学视角拆解AI的天花板，提供一个比"算力不够"更深刻的解释框架

---

## AI的瓶颈，不在算力

有一个问题，Dwarkesh Patel在采访了一个又一个AI研究员之后，始终没有得到令他满意的答案：我们往大模型里塞进了远超任何人类一生所能消化的数据，它们却依然只有人类能力的一小部分。这到底是怎么回事？

Adam Marblestone的回答没有绕弯子："这可能是一个价值万亿美元的问题，甚至可以说是科学史上最重要的问题。我不敢说自己知道答案。"

但他接下来说的，比一个确定的答案更有价值——他给出了一个框架，解释了为什么这个问题如此难，以及我们现在的研究方向可能在哪里系统性地走偏了。

Marblestone是神经科学出身，后来深度介入AI研究。这个双重背景让他能站在一个特殊的位置上看问题：他既理解深度学习的内部逻辑，也对大脑的生物机制有第一手的认知。他的核心判断是：当前AI研究严重低估了一个变量——代价函数（cost function）的具体性。

### 数学上简洁的损失函数，可能是个陷阱

在深度学习的框架里，训练一个模型需要四个要素：架构、超参数、学习算法，以及代价函数——也就是你在训练它做什么、用什么信号来奖惩它。

机器学习领域有一种根深蒂固的偏好：数学上简洁的损失函数。预测下一个词，交叉熵损失，干净、优雅、可扩展。GPT系列的成功在某种程度上强化了这种偏好——如果一个足够简单的目标加上足够多的数据和算力能产生如此惊人的结果，为什么要把事情复杂化？

Marblestone的回答是：因为大脑不是这么工作的。

"大脑有极度具体的、生物学上实例化的代价函数，这些函数经过了数亿年的进化塑造。大脑不是在做下一个词预测。它在做某种更具体的事情，而我认为这种具体性至关重要。"

这不是一个模糊的哲学论断。他指向了一个非常具体的例子：多巴胺系统。

### 多巴胺：我们没有它的计算模型

在大众认知里，多巴胺是"奖励物质"。在强化学习的框架里，它被类比为奖励信号。这个类比帮助神经科学和AI互相借鉴了很多年，但Marblestone认为它严重简化了现实。

"多巴胺已经被证明能编码预测误差，但同时也能门控可塑性、控制学习的时机、调节注意力。它同时在做很多事情，而我们对此没有好的计算模型。"

这里的关键词是"同时"。多巴胺不是一个单一功能的信号，它是一个多维度的调制系统。它不只告诉神经元"这个好"或"这个不好"，它还在控制哪些突触应该在这个时刻发生变化，学习的窗口应该开多久，注意力应该分配到哪里。

同样的逻辑适用于血清素、乙酰胆碱、去甲肾上腺素。这些神经调质（neuromodulators）构成了一套复杂的元学习系统——它们不是在直接处理信息，而是在调控信息处理的方式本身。

当前的AI系统有类似的机制吗？基本没有。注意力机制（attention）在某种程度上是一个粗糙的类比，但它是静态的、在训练后固定的，而不是像神经调质那样动态地响应当前的生理和认知状态。

### AI不睡觉，这可能是个大问题

Marblestone提出的第二个被低估的因素是睡眠。

"在睡眠期间，大脑在做一些我们还没有完全理解的事情，但这些事情对于巩固记忆、修剪突触、清除代谢废物似乎至关重要。当前的AI系统不睡觉。它们没有这个离线巩固阶段。我认为这可能是缺失的关键成分之一。"

从计算的角度来看，睡眠可能是大脑在做某种形式的离线优化——在没有新输入干扰的情况下，重新整理白天获得的信息，强化重要的连接，弱化不重要的连接。这不是简单的"保存文件"，更像是在夜间对整个知识结构进行重新索引和压缩。

当前的大模型训练完成后就是固定的。它们没有持续学习的机制，更没有离线整合的阶段。每次"学习"新知识都需要重新训练，而且往往会遭遇灾难性遗忘（catastrophic forgetting）——学了新的，忘了旧的。大脑在这方面的表现要稳健得多，而睡眠可能正是其中的关键机制。

### 没有身体的文字处理器

第三个缺口是具身性（embodiment）。

"大脑在一个在世界中移动的身体里进化。它有本体感觉、前庭感觉、内感受——对身体内部状态的感知。这些不是外围输入，它们深度整合进了认知。当你思考'理解'意味着什么，很大一部分是植根于身体经验的。当前的大语言模型没有这些。它们是去身体化的文字处理器。"

当一个人理解"热"这个词，背后有皮肤感受器的记忆、有被烫到时的退缩反应、有体温调节系统的整体经验。当一个LLM处理"热"这个词，它有的是这个词在数十亿文本中的统计共现模式。这两种"理解"是同一种东西吗？

Marblestone的立场是：不是。而且这个差距不是通过更多数据或更大模型能弥合的，因为问题出在输入的性质上，而不是输入的数量上。

### 神经科学需要一个"人类基因组计划"

面对这些缺口，Marblestone的处方不是"让AI研究员更努力地思考大脑"，而是"让神经科学本身变成一个更强大的领域"。

"我总体的元层面判断是，我们必须赋能神经科学领域，让它在技术上和其他方面都变得更强大，才能真正破解这样的问题。"

他给出了一个具体的参照系：人类基因组计划花费了约30亿美元，彻底改变了生物学。他认为神经科学需要类似量级的协同投入——系统性地绘制连接组（connectome）、理解神经回路的动态、搞清楚大脑实际上在计算什么。

### 在当前范式耗尽之前

Marblestone对AI现状的评估是清醒的："当前AI系统是脆弱的。它们以人类不会失败的方式失败。它们无法可靠地进行常识推理。它们无法像人类那样从少量例子中学习。它们没有真正的理解。我认为原因是我们对大脑的工作方式缺失了某些根本性的东西。"

问题是：我们能在当前范式耗尽之前找到那个缺失的东西吗？

他的回答是"谨慎乐观"。但这种乐观是有条件的——它需要对基础神经科学的投入远超过我们目前的水平。

当整个行业都在讨论scaling law、下一代架构、更大的训练集时，Marblestone在说：也许我们需要先退一步，真正搞清楚我们在模仿的那个系统到底是怎么工作的。
