1. "DeepSeek R1 的出现不只是一个技术事件，它给了中国所有科技公司一个平台，让他们开始发布真正有竞争力的开放权重模型。"
2. "预训练的规模化还没死——它只是变贵了。真正的问题是：你愿意把计算预算花在训练上，还是推理上？"
3. "RLVR 的核心魔法：你只需要给模型一道题和正确答案，它会自己想办法推导出来。50步之内，准确率从15%跳到50%。"
4. "同事评价比上司评价更准确——这是98%的人都同意的事实。但当你问谁应该做绩效评估时，大多数人还是说上司。"
5. "语言模型最难做到的事，是真正触及深刻洞见。RHF 的本质是对大量人类偏好取平均，而平均值天然会磨平锐角。"

📝 创作说明
嘉宾：Sebastian Raschka、Nathan Lambert
选题方向：ML研究员 Sebastian Raschka & Nathan Lambert + 2026年AI技术全景盘点
内容评分：信息密度 8/10，数据支撑 7/10，洞见深度 8/10
目标字数：2100 字
核心价值：两位顶级ML研究员对2026年AI技术现状的深度拆解，覆盖模型竞争、架构演进、训练范式、开源生态与教育方法论

## DeepSeek 之后，中国开源生态全面爆发

2025年1月，DeepSeek R1 发布。这件事的意义不只是"一家中国公司做出了接近最先进水平的模型"，而是它触发了一场连锁反应。

Nathan Lambert 的观察是：DeepSeek 之于中国AI，就像 ChatGPT 之于美国AI——它给了整个生态一个信号，让大量公司开始认真发布开放权重模型。Kimi（Moonshot）、MiniMax、Z.AI 的 GLM 系列，在过去几个月里都展现出越来越强的竞争力。DeepSeek 本身反而有点"失去了作为最杰出中国开放模型的桂冠"。

Sebastian Raschka 补充了一个有趣的结构性观察：这些中国公司发布开放权重模型，有一个非常务实的商业逻辑——美国和欧洲的企业不会为了安全顾虑去订阅中国公司的 API，但他们可以下载开放权重模型在本地运行。开放权重是绕过信任壁垒的方式，同时也在全球范围内积累影响力。

另一个被低估的因素是许可证。Meta 的 LLaMA 和 Google 的 Gemma 都附带使用限制（超过一定用户数需要报告财务状况等），而很多中国模型采用更宽松的许可证，"没有附加条件"。这在开发者社区里是真实的竞争优势。

## 架构没有根本改变，但调参的艺术越来越精

从 GPT-2 到今天的前沿模型，变压器（Transformer）架构的核心没有根本性变化。Sebastian 做了一个清醒的梳理：

混合专家（MoE）不是 DeepSeek 发明的，分组查询注意力（GQA）也不是，RMS 归一化替代层归一化是个小调整，非线性激活函数的替换也只是微调。"你可以从一个架构迁移到另一个，把这些组件加进去，它们基本上还是同一个架构。"

真正的变化发生在训练范式上。预训练之后，现在有了明确的"训练中期"（mid-training）和"训练后"（post-training）阶段。训练中期专注于特定能力的强化，比如长上下文处理；训练后则是解锁模型已有知识的技能——监督微调、DPO、RLVR。

Nathan 的核心论点是：**预训练给模型注入知识，训练后解锁技能**。RLVR 不是在教模型新的数学知识，而是在解锁它预训练时已经吸收的能力。证据是：用 RLVR 在 Qwen3 基础模型上训练50步，准确率从15%跳到50%——这种速度不可能是在学习新知识。

## RLVR：给模型一道题，让它自己想办法

RLVR（Reinforcement Learning with Verifiable Rewards，可验证奖励强化学习）是2025年最重要的训练范式突破。

原理很简单：给模型一道有明确答案的题（数学、编程、事实问答），让它尝试很多次，根据答案是否正确给予奖励，然后用强化学习梯度更新模型权重。不需要人工标注推理过程，只需要题目和答案。

DeepSeek R1 论文里有一个被广泛引用的"顿悟时刻"（aha moment）：模型在训练过程中自发学会了回溯和自我纠正——"等等，我好像算错了，让我重新试试。"这个行为没有被显式训练，是从可验证奖励中自然涌现的。

Nathan 的团队在 AI2 的工作（Tulu 3）实际上比 DeepSeek 更早使用了 RLVR 这个术语，但 DeepSeek 让它广为人知。他们的实验数据：用约300亿参数的模型，在截止日期前5天跑 RLVR，再多跑3.5周，模型有了显著提升——"那可是一大笔计算预算，但值得。"

RLVR 的计算需求与预训练不同：预训练是密集的矩阵乘法，GPU 之间需要高带宽通信；RLVR 更依赖内存，因为需要生成长序列，注意力机制的内存消耗随序列长度二次增长。Grok 4 据称在训练后阶段使用了与预训练相当的计算量。

## 预训练规模化：还活着，但变贵了

关于预训练规模化是否"死了"，两人的答案是：没有，但它的性价比在下降。

Nathan 的框架是：预训练是固定成本，训练一次，永久获得能力；推理时间缩放是变动成本，每次查询都要花钱。如果你的模型只用一年就会被替换，花1亿美元做更长的预训练可能不如把这笔钱用在推理上。

Sebastian 补充了一个数据点：Qwen 系列据报道使用了约50万亿 token 的预训练数据，而研究用小模型通常在5到10万亿 token。封闭实验室可能达到100万亿 token。这个量级的差距，加上数据质量的提升（合成数据、OCR 提取的 PDF、精心筛选的混合比例），仍然能带来显著的模型能力提升。

2026年的一个关键变量是 Blackwell 集群的上线。大量千兆瓦级数据中心正在建设，这些计算资源会被用于更大规模的预训练。Nathan 预测今年可能会出现2000美元/月的订阅套餐——对应更大、更强的模型。

## 工具调用：被低估的范式转变

Nathan 特别强调了工具调用（tool use）作为一个被开源生态低估的能力。

他的论点是：幻觉问题的最佳解决方案不是让模型记住更多，而是让它知道什么时候该用工具。"为什么要让模型记住1998年世界杯冠军是谁？直接搜索就好了。"

GPT OSS 是第一个真正经过工具调用训练的开放权重模型，这是它的核心价值。但工具调用在开源生态中还没有被充分利用，主要障碍是信任——你不想让一个可以访问你文件系统的模型随意执行命令。

他预测工具调用将是未来几年最重要的能力解锁之一，尤其是结合递归语言模型（Recursive LM）的思路——让模型自己决定何时分解子任务、何时调用工具、何时压缩上下文。

## 从零构建：学习 LLM 的最佳方式

Sebastian 的两本书——《从零开始构建大型语言模型》和《从零开始构建推理模型》——体现了他对学习方法论的核心信念：**代码不会撒谎**。

数学公式可以有错误，解释可以被误解，但代码要么跑通，要么跑不通。从零实现一个 GPT-2，然后逐步添加 MoE、GQA、RoPE 等组件，每一步都可以对照 Hugging Face Transformers 库的参考实现进行单元测试。"最后你知道你做对了，因为输出完全匹配。"

Nathan 补充了一个实用建议：阅读技术书籍时，先做一遍离线专注阅读，不要立刻跳到 Twitter 和博客去追热点。第二遍再用 LLM 补充背景、解释概念。"一旦你掉进 Twitter 的兔子洞，你就不再是在学习了，你是在消费互联网。"

两人都认为，在 LLM 时代，学习编程的乐趣并没有消失，只是形态变了。Sebastian 描述了调试的快感："找到那个 bug 的瞬间，是世界上最好的感觉。"但如果你把所有调试都外包给 LLM，你永远不会有这种感觉。

## 研究者的职业选择：学术界 vs 工业实验室

对于想进入 AI 研究领域的人，Nathan 给出了一个清醒的框架：

学术界的优势是发表自由、长期研究方向的自主权、以及可以公开讨论工作。劣势是计算资源有限、资金压力大（尤其是当前政府削减拨款的环境下）。

工业实验室（OpenAI、Anthropic、Google DeepMind 等）的优势是计算资源充足、薪酬极高（OpenAI 员工平均股票薪酬据报道超过100万美元/年）、能产生大规模影响。劣势是越来越封闭，发表越来越少，你是"机器里的一个齿轮"。

他的建议是：如果你想在计算资源有限的情况下做出贡献，专注于评估（evaluation）是性价比最高的方向。找到一个前沿模型失败的具体场景，写出高质量的评估报告，让 Frontier Lab 采用你的评估——这是一张职业火箭船票。

"你不需要拥有每个项目都这样做的计算资源。但如果你找到了 Claude 4.5 在某个方面的失败模式，并在博客文章里展示出来，下一个 Claude 模型就会在发布说明里提到这件事。"
