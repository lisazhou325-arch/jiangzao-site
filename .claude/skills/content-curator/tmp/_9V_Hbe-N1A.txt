the big million-dollar question that I have that um I've been trying to get the answer to through all these interviews with AI researchers. How does the brain do it? Right? Like we're throwing way more data at these LLMs and they still have a small fraction of the total capabilities that a human does. So what's going on? &gt;&gt; Yeah. I mean this might be the quadrillion dollar question or something like that. It's it's it's arguably you can make an argument this is the most important you know question in science. I don't claim to know the answer. I I also don't really think that the answer will necessarily come even from a lot of smart people thinking about it as much as they are. I my my overall like meta level take is that we have to empower the field of neuroscience to just make neuroscience a a more powerful uh field technologically and otherwise to actually be able to crack a question like this. But maybe the the way that we would think about this now with like modern AI, neural nets, deep learning is that there are sort of these these c certain key components of that. There's the architecture. Um there's maybe hyperparameters of the architecture. How many layers do you have or sort of properties of that architecture? There is the learning algorithm itself. How do you train it? You know, back prop, gradient descent, um is it something else? There is how is it initialized? Okay, so if we take the learning part of the system, it still may have some initialization of of the weights. Um, and then there are also cost functions. There's like what is it being trained to do? What's the reward signal? What are the loss functions, supervision signals? My personal hunch within that framework is that the the field has neglected uh the role of the very specific loss functions, very specific cost functions. uh machine learning tends to like mathematically simple loss functions right predict the next token um you know cross entropy the these these these these uh simple kind of computer scientist loss functions I think evolution may have built a lot of complexity into the loss functions actually many different loss functions were different areas turned on at different stages of development a lot of Python code basically uh generating uh a specific curriculum for what different parts of the brain need to learn because evolution has seen many times what was successful and unsuccessful and evolution could encode the knowledge of of the learning curriculum. So, so in the in the machine learning framework maybe we can come back and we can talk about yeah where do the loss functions of the brain come from can that can loss different loss functions lead to different efficiency of learning &gt;&gt; you know people say like the cortex has got the universal human learning algorithm the special size that humans have &gt;&gt; what's up &gt;&gt; this is a huge question uh and we don't know I've seen models where what the cortex uh you know the cortex has typically this like six layered structure layers in a slightly different sense than layers of a neural net it's like any one location in the cortex has six physical layers of tissue as you go in layers of the sheet and then those areas then connect to each other and that's more like the layers of a network. Um I've seen versions of that where what you're trying to explain is actually just how does it approximate back prop and what is the cost function for that what is the network being asked to if you sort of are trying to say it's something like back prop is it doing backrop on next token prediction is it doing back prop on uh classifying images or or what is it doing &gt;&gt; um and uh no one no one knows um but I think I think one one thought about it one possibility about it is that um is just this incredibly general um prediction engine. So, so any one area of cortex is just trying to predict any basically can it learn to predict any subset of all the variables it sees from any other subset. So like omniirectional inference um or omniirectional prediction um whereas an LLM is just you see everything in the context window and then it it computes a very particular &gt;&gt; conditional probability which is given all the last thousands of things what is the very probabilities for all the all the the next token. &gt;&gt; Yeah. Um, but it would be weird for a large language model to say, you know, um, you know, the quick brown fox, blank blank, the lazy dog, um, and fill in in the middle. Yeah. Um, &gt;&gt; uh, versus do the next token. It, if it's, if it's doing just forward, it can learn how to do that stuff in this emergent level of in context learning, but natively it's just predicting the next token. What if the cortex is just natively made so that it can you know any area of cortex can predict any pattern in any subset of its inputs given any other missing subset. Um that is a little bit more like quote unquote probabilistic AI. Um I think a lot of the things I'm saying by the way are extremely similar to like what Yan Lun would say. &gt;&gt; Yeah. &gt;&gt; Um he's really interested in these energy based models. Um and something like that is like the joint distribution of all the variables. What is the what is the likelihood or unlikelihood of just any combination of variables? And if I if I clamp some of them, I say well definitely these variables are in these states. Then I can compute with probabilistic sampling for example I can compute okay conditioned on these being set in this state. What are and these could be any arbitrary subset of of of variables in the model. Can I predict what any other subset is going to do and sample from any other subset given clamping this subset and I could choose a totally different subset and sample from that subset. Um so it's omnidirectional inference and so you know that could be there's some parts of of cortex that might be like association areas of cortex that may you know predict vision from audition. &gt;&gt; Yeah. There might be areas that predict things that the more innate part of the brain is going to do because remember this whole thing is basically riding on top of the sort of a lizard brain and lizard body if you will. Um, and that thing is a thing that's worth predicting too. So you're not just predicting do I see this or do I see that but is this muscle about to tense? Am I about to have a reflex where I laugh? You know, is my heart rate about to go up? Um, am I about to activate this instinctive behavior &gt;&gt; based on my higher level understanding of like I can match &gt;&gt; uh somebody has told me there's a spider on my back &gt;&gt; to this lizard part that would activate if I was like literally seeing a spider in front of me and you you learn to associate the two so that even just from somebody hearing you say there's a spider on your back. &gt;&gt; Yeah. Let's well let's come back to this and this this is partly having to do with with Steve Baron's theories which I'm recently obsessed about but but on your podcast with Ilia um &gt;&gt; he said look I'm not aware of any any good theory of how evolution encodes highle desires or intentions. I think this is like this is like very connected uh to to to all of these questions about the loss functions and the cost functions um that the brain would use and it's a really profound question right like like let's say that um I am embarrassed for saying the wrong thing on your podcast because I'm imagining that young lun is listening and he says that's not my theory that you describe energy based models really badly that's going to inact activate in me innate embarrassment and shame and I'm going to want to go hide and and whatever and that's going to activate these innate reflexes. Um, and that's important because I might otherwise get get killed by Yan Lun's, you know, [laughter] marauding army of of other &gt;&gt; the Frenchi researchers coming for you, Adam. &gt;&gt; And so it's important that I have that instinctual response. But of course, evolution has never seen Yan Lun or known about energy based models or known that what a important scientist or podcast is. And so somehow the brain has to encode this desire to you know uh not not piss off really important you know people in the tribe or something like this um in a very robust way without knowing in advance all the things that the the learning subsystem okay of the brain the part that is learning cortex and other parts uh the cortex is going to learn this world model that's going to include things like yan lun and and podcasts and uh evolution has to make sure that that those neurons, whatever the Yan Lun being upset with me neurons get properly wired up to the shame response or this part of the reward function. Um, and this is important, right? Because if we're going to be able to seek status in the tribe or learn from knowledgeable people as you said or things like that, exchange knowledge and skills with friends but not with enemies. I mean, we have to learn all this stuff. So it has to be able to robustly wire these learned features of the world um learned parts of the world model up to uh these innate reward functions and then actually use that to then learn more. Right? Because next time I'm not going to try to piss off Yan Lun if he emails me that that I got this wrong. Um and so uh we're going to do further learning based on that. So in constructing the reward function, it has to use learned information. But how can evolution evolution didn't know about young lun. How can how can it how can it do that? And so uh the basic idea um that Steve Burns is proposing is that well part of the cortex uh or or other areas like the amydala that learn um what they're doing is they're modeling the steering subsystem steering subsystem is the part with these more innate innately programmed responses and the innate programming of these series of reward functions cost functions bootstrapping uh functions that exist. So there are parts of the amydala for example that are able to monitor what what those parts do and predict what those parts do. So um so so how do you find the neurons um that are important for social status? Well, you have some innate heristics of social status for example or you have some innate uh innate uh heruristics of friendliness that um that the steering subsystem can use and the steering sub actually has its own sensory system which is kind of crazy. So we think of you know vision as being something that the cortex does. &gt;&gt; But there's also a steering subsystem subcortical visual system called the superior caliculus with innate ability to detect faces for example or threats. Um so it so there's a visual system that uh has innate heristics um and that the steering subsystem has its own responses. So there'll be part of the amydala or part of the cortex that is learning to predict those responses. And so what are the neurons that that matter in the cortex for um social status or for friendship or they're the ones that predict those innate heristics for friendship, right? So you train a predictor in the cortex and you say which neurons are part of the predictor. Uh those are the ones that are now you've actually managed to wire it up. Yeah, &gt;&gt; this is fascinating. Um I I feel like I still don't understand I understand how the cortex could learn how this primitive part of the brain would respond to um so it can obviously it has these labels on here's literally a picture of a spider and this is bad like be scared of this &gt;&gt; and then the cortex learns that this is bad because the innate part tells it that but then &gt;&gt; it has to generalize to okay if the spider's on my back &gt;&gt; yes &gt;&gt; and somebody's telling me the spider's on your back that's also bad. &gt;&gt; Yes, &gt;&gt; but it never got supervision on that. &gt;&gt; So, how does it &gt;&gt; Well, it's because the learning subsystem um is a powerful learning algorithm that does have generalization uh that is capable of generalization. So, the steering subsystem, these are the innate responses. So, you're going to have some let's say um built into your steering subsystem uh these lower brain areas, hypothalamus, brain stem etc. um and again they include they have their own primitive sensory systems. So, there may be an innate response. Um, if I see something that's kind of moving fast toward my body that I didn't previously see was there and is kind of small and dark and high contrast, that might be an insect kind of skittering onto my body. Um, I am going to like flinch, right? &gt;&gt; Um, and so there are these innate responses. And so there's going to be some group of neurons, let's say, in the hypothalamus that is the I am flinching. &gt;&gt; Yeah. &gt;&gt; Uh, or I just flinched, right? Right. they're the the the I I just flinched neurons in the hypothalamus. Um so when you flinch first of all that a negative contribution to the reward function. You didn't want that to happen perhaps. Um but that's only hap that's a reward function then that is it doesn't have any generalization in it. So I'm going to avoid that exact situation of the thing skittering toward me. &gt;&gt; Um &gt;&gt; and maybe I'm going to avoid some actions that lead to the thing skittering. Uh so that's that's something a generalization you can get. What Steve calls it is downstream of the reward function. Um, so I'm going to avoid the situation where the spider was skittering toward me. Um, but you're also going to do something else. So there's going to be like a part of of your amydala, say that is saying, okay, um, a few, you know, a few milliseconds, you know, hundreds hundreds of milliseconds or seconds earlier. &gt;&gt; Um, could I have predicted that flinching response? It's going to be it's going to be a group of neurons that is essentially a classifier of am I about to flinch? Um, and I'm going to have classifiers for that for every important steering subsystem variable that evolution needs to take care of. Am I about to flinch? Am I talking to a friend? Should I laugh now? Is the friend high status? Whatever variables the hypothalamus brain stem contain. Um, am I about to taste salt? Um, so it's going to have uh all these variables and for each one is going to have a predictor. It's going to train that predictor. Now the predictor that it trains that can have some generalization and the reason it can have some generalization is because it just has a totally different input. So its input data might be things like the word spider, right? But the word spider can activate in all sorts of situations that lead to the world word spider activating in your word world model. Um so you know if you have a a complex world model which really complex features that inherently gives you some generalization. It's not just the thing skittering toward me. It's even the word spider uh or the concept of spider is going to cause that to trigger. And this predictor can learn that. So, whatever spider neurons are in my world model, um, which could even be a book about spiders or somewhere a room where there are spiders or whatever that is. Um, &gt;&gt; the amount of heebiejeebies that this this conversation is listening in the audience is like, &gt;&gt; so now I'm activating your steering subsystem, [laughter] your your uh steering subsystem, spider, hypothalamus, subgroup of neurons of of skittering insect are activating based on these very abstract concept in the conversation. &gt;&gt; You get going. I'm going to put in a trigger warning. [laughter] That's because that's because you learned this and the the cortex inherently has the ability to generalize because it's just predicting based on these very abstract variables and all these integrated information that it has whereas the the steering system only can use whatever the superior click list and a few other sensors can spit out. So &gt;&gt; by the way it's remarkable that the person who's made this this connection between different pieces of neuroscience Steven Burns like former physicist has for the last few years has been trying to synthesize. &gt;&gt; He's an AI safety researcher. He's just synthesizing. This comes back to the academic incentives thing. I think that this is it's this is a little bit hard to say what is the exact next experiment? How am I going to publish a paper on this? How am I going to train my grad student to do this? It's very very speculative, but there's a lot in the neuroscience literature and Stephen has been able to pull this together. And I think that Steve has an answer to Ilio's question essentially, which is which is how how does the brain &gt;&gt; ultimately code for these higher level desires and link them up to the more primitive rewards. Yeah, &gt;&gt; very naive question. But why can't we achieve this omniirectional inference by just training the model to not just map from a token to next token but remove the masks in the training so it maps every token to every token or um come up with more labels between video and audio and text so that it it's forced to map one to each one. &gt;&gt; I mean that may be that may be the way. So it's it's not clear to me. Some people think that there's sort of a different way that it does probabilistic inference or a different learning algorithm that isn't back prop. um there might be like other ways of learning energy based models or other things like that that you can imagine but uh that is involved in being able to do this and that the brain has that but I think there's a version of it where you know the what the brain does is like crappy versions of back prop to learn to predict you know through a few layers and that yeah it's it's kind of like a multimodal foundation model right &gt;&gt; yeah so maybe the cortex is just kind of like a certain kinds of foundation models there you know LLMs are maybe just predicting the next token but you know vision models maybe are train learning to fill in the blanks or reconstruct different pieces or combinations. But but I think that uh it does it in an extremely flexible way. So it's, you know, if you train a model to just fill to fill in this blank at the center, okay, that's great. But what if you didn't train it to to fill in this other blank over to the left? Um then it doesn't know how to do that. It's not part of its like uh repertoire of predictions that are like immortized into the network. Whereas with a really powerful inference system, you could choose at test time, you know, what is the the sub, you know, the the subset of variables it needs to infer and what which ones are clamped. &gt;&gt; Okay, two questions. one I it makes you wonder whether the thing that is lacking in artificial neural networks is less about the reward function and more about the encoder or the embedding which um like maybe the issue is that you're not representing video and audio and text in the right latent abstraction such that they could intermingle and um conflict. Maybe this is also related to why LLM seem bad at drawing connections between different ideas. Like it's like are the ideas represented at a level of generality at which you could which you could notice different. &gt;&gt; The problem is these questions are all co-ingle. So if we don't know if it's doing a back prop like learning and we don't know if it's doing energy based models and we don't know how these areas are even connected in the first place. It's like very hard to like really get to the ground truth of this. But yeah, it's possible. I mean I think that people have done some work. My friend Joel Depello actually did something some years ago where um I think he put a model I think it was a model of V1 um of sort of specifically how the the early visual cortex represents images and put that as like an input into like a connet and that like improves some things. So it could be it could be like differences the retina is also doing you know motion detection and certain things are kind of getting filtered out. So there there may be some pre-processing of the sensory data. There may be some clever combinations of which modalities are predicting which or so on that um that lead to better representations. There may be much more clever things than that. Some people certainly do think that there's inductive biases built into the architecture that will shape the representations you know differently or that there are clever things that you can do. Um so Astera which was the this the same organization that employs Steve Burns just launched this neuroscience project based on Doris so uh work and she has some ideas about how you can build um vision systems that basically require less training they put in they inbuild into the assumptions of the design of the architecture um that uh things like objects are bounded by surfaces and this you know surfaces have certain types of shapes and relationships of how they olude each and stuff like that. So it may be possible to build more assumptions into the network and evolution may have also put some changes of architecture. Um it's just I think that also the cost functions and so on may be a a key a key thing that it does. &gt;&gt; So Andy Jones has this amazing 2021 paper where he uses Alpha Zero to show that you can trade off testime compute and training compute. And while that might seem obvious now, this was 3 years before people were talking about inference scaling. So, this got me thinking, is there an experiment you could run today, even if it's a toy experiment, which would help you anticipate the next scaling paradigm? One idea I had was to see if there was anything to multi- aent scaling. Basically, if you have a fixed budget of training compute, are you going to get the smartest agent by dumping all of it into training one single agent or by splitting that compute up amongst a bunch of models, resulting in a diversity of strategies that get to play off each other? I didn't know how to turn this question into a concrete experiment, though. So, I started brainstorming with Gemini 3 Pro in the Gemini app. Gemini helped me think through a bunch of different judgment calls. For example, how do you turn the training loop from selfplay to this kind of co-evolutionary league training? How do you initialize and then maintain diversity amongst different Alpha Zero agents? How do you even split up the compute between these agents in the first place? I found this clean implementation of AlphaGo Zero, which I then forked and opened up in anti-gravity, which is Google's Asian first IDE. The code was originally written in 2017 and it was meant to be trained on a single GPU of that time, but I needed to train multiple whole separate populations of Alpha Zero agents. So, I needed to speed things up. I rented a beef cake of a GPU node, but I needed to refactor the whole implementation to take advantage of all this scale and parallelism. Gemini suggested two different ways to parallelize selfplay. One which would involve higher GPU context switching and the other would involve higher communication overhead. I wasn't sure which one to pick, so I just asked Gemini. And not only did it get both of them working in minutes, but it autonomously created and then ran a benchmark to see which one was best. It would have taken me a week to implement either one of [music] these options. Think about how many judgment calls a software engineer working on an actually complex project has to make. If they have to spend weeks architecting some optimization or feature before they can see whether it will work out, they will just get to test out so many fewer ideas. Anyways, with all the help from Gemini, I actually ran the experiment and got some results. Now, please keep in mind that I'm running this experiment on an anemic budget of compute and it's very possible I made some mistakes in implementation. But it looks like there can be gains from splitting up a fixed budget of training compute amongst multiple agents rather than just dumping it all into one. Just to reiterate how surprising this is, the best agent in the population of 16 is getting 116th the amount of training compute as the agent trained on selfplay alone. And yet it still outperforms the agent that is hogging all of the compute. The whole process of vi coding this experiment with Gemini was really absorbing and fun. It gave me the chance to actually understand how Halzero works and to understand the design space around decisions about the hyperparameters and how search is done and how you do this kind of co-evolutionary training rather than getting bogged down in my very novice abilities as an engineer. Go to gemini.google.com to try it out. I want to talk about this idea that you just uh glanced off of which was amvertised inference. Um, and maybe I should try to explain what I think it means cuz I think it's probably wrong and this this will help you correct. &gt;&gt; It's been a few years for me too. So, &gt;&gt; okay. Um right now the way the models work is you have an input it maps it to an output and this is amortizing a process that the the real process which we think is like what intelligence is which is like you have some prior over how the world could be like what are the causes that make the work world the way it is and then the way when you see some observation you should be like okay here's all the ways the world could be um this cause explains what's happening best. Now the like doing this calculation over every possible cause is computationally intractable. So then you just have to sample like oh here's a potential cause does this explain this observation? Uh no forget it let's let's keep sampling and then eventually you get the cause the cause then the cause explains the observation and then this becomes your posterior. That's actually pretty good I think of sort of yeah so yeah this basian inference like in general is like of this very intractable thing &gt;&gt; right &gt;&gt; it the algorithms that we have for doing that tend to require taking a lot of samples Monte Carlo methods taking a lot of samples &gt;&gt; and taking samples takes time I mean this is like the original like boltzman machines and stuff were using techniques like this &gt;&gt; um &gt;&gt; and still it's used with probabistic programming other types of methods often and so uh yeah so the basian inference problem which is like basically the problem of like perception like given some model of the world and given some data like how should I update my how what what are the like the variables you know missing variables in my in my internal model &gt;&gt; and I guess yeah the idea is that neural networks are hopefully um obviously there's mechanistically the neural network is not starting with like here is my model of the world and I'm going to try to explain this data but the hope is that instead of starting with um hey does this cause explain exaggeration Oh, here did this cause explain this explanation? Yes. What you do is just like observation &gt;&gt; what's the most what's the cause that we the neural net thinks is is the best one. &gt;&gt; Observation to cause. So the feed forward like goes observation to cause &gt;&gt; observation to cause &gt;&gt; to the output. &gt;&gt; Yes. You don't have to you don't have to evaluate all these energy values or whatever and and sample around to make them higher and lower. &gt;&gt; Um you just say um &gt;&gt; approximately that process would result in this being the top one or something like that. &gt;&gt; Yeah. One way to think about it might be that test time compute inference time compute is actually doing this sampling again because you literally read its chain of thought. It's like actually doing this toy example we're talking about where it's like oh can I solve this problem by doing X? Nah, I need a different approach. And this raises the question I mean over time it is the case that the capabilities which were &gt;&gt; uh which required inference time compute to elicit get distilled into the model. So you're amortizing the thing which previously you needed to do these like rollouts these like Monte Carlo rollouts to um to figure out. And so in general there maybe there's this principle of digital minds which can be copied have different trade-offs which are relevant than biological minds which cannot. &gt;&gt; And so in general it should make sense to amortize more things because you can literally copy the copy the amortization right or copy the things that you have um &gt;&gt; sort of like built in. Yeah. &gt;&gt; Um and it this is a tangential question where it might be interesting to speculate about in the future as these things become more intelligent and the way we train them becomes more economically rational. What will make sense to amortize into these minds which evolution did not think it was worth amortizing into biological minds. They did you have to retrain every time &gt;&gt; right? I mean, first of all, I think the probabistic AI people would be like, of course, you need test time compute because this inference problem is really hard and the only ways we know how to do it involve lots of test time compute. Otherwise, it's just this crappy approximation that's never going to like you have to do infinite data or something to like make this. &gt;&gt; So, I think some of the probabistic people will be like, no, it's like inherently probabistic and like amortizing it in this way like just doesn't make sense. And so, and they might then also point to the brain and say, "Okay, well, the brain, the neurons are kind of stoastic and they're sampling and they're doing doing things." And so, maybe the brain actually is doing more like the non-amortized inference, the real inference. Um, but it's also kind of strange how perception can work in just like milliseconds or whatever. It doesn't seem like it uses that much sampling. So, it's also clearly also doing some kind of um baking things into into like approximate forward passes or something like that to do this. And yeah, so in the future, you know, I don't know. I mean, I think is it already a trend to some degree that things that are people are having to use test time compute for are getting like used to train back the the base model, right? &gt;&gt; Yeah. &gt;&gt; Yeah. That so now it can do it in one pass, right? &gt;&gt; Yeah. So I mean I think &gt;&gt; Yeah. You know, maybe evolution did or didn't do that. uh I think evolution still has to pass everything through the genome right to build the network so and the environment in which humans are living is very dynamic right and so maybe that's if we believe this is true that that there's a learning subsystem per Steve Burns and a steering subsystem the the learning subsystem doesn't have a lot of like pre-initialization or pre-training um it has a certain architecture but then within lifetime it learns um then evolution didn't you know actually like immortize that much into that network. It immortizes it instead into a set of innate behaviors and a set of these bootstrapping cost functions or ways of building up very particular reward signals. Yeah, &gt;&gt; this framework helps explain this um mystery that people have pointed out and I've asked a few guests about which is h um if you want to analogize evolution to pre-training, well, how do you explain the fact that so little information is conveyed through the genome? So 3 GB is the size of the total human genome. Obviously a small fraction of that is actually relevant to coding at the brain. &gt;&gt; Yeah. &gt;&gt; Um and if previously people made this analogy that actually pre evolution has found the hyperparameters of the model the the numbers which tell you how many layers should there be the architecture basically right like what what how should things be wired together. But if a big part of the story that increases sample efficiency aids learning generally makes systems more performant is the reward function is the loss function. Yeah. &gt;&gt; So you can like build an intelligence with so little information because like the reward function you like write in Python, right? The reward function is like literally a line and so you just like have like a thousand lines like this and that's doesn't take up that much space. &gt;&gt; Yes. And it also gets to do this generalization thing with the the thing the thing I was describing where we were talking with about the spider, right? where it learns that just the word spider, you know, triggers the spider, you know, reflex or whatever. Um, it gets to exploit that too, right? So, it gets to build a reward function that actually has a bunch of generalization in it just by specifying these innate spider stuff and the thought assessors as Steve calls them that do the learning. Um, so that's like potentially a really compact solution um to building up these more complex reward functions too that you need. So it doesn't have to anticipate everything about the future of the reward function. Just has to anticipate what variables are relevant and what are heristics for like finding what those variables are. Um and then yeah so then it has to have like a very compact specification for like the learning algorithm and basic architecture of the learning subsystem. And then it has to specify all this Python code of like all the stuff about the spiders and all the stuff about friends and all the stuff about your mother and all the stuff about meeting and um and and and social groups and joint eye contact. It has to specify all that stuff. Um and so is this really true? And so I think that there is some evidence for it. So so so Fech Chen and and Evan McCosco and various other researchers who have been doing like these single cell atlases. So one of the things that um neuroscience technology or I'm scaling up neuroscience technology again this is kind of like my one of my obsessions um has done uh through through um the brain initiative a big you know neuroscience funding program is they've basically gone through different areas especially of the mouse brain and map like where are the different cell types um how many different types of cells are there in different areas of cortex are they the same across different areas and then you then you look at these subcortical regions which are more like the like steering subsystem or reward function generating regions. How many different types of cells do they have and which neurons types do they have? We don't know how they're all connected and exactly what they do or what the circuits are, what they mean, but you can just like quantify like how many different kinds of cells are there um with sequencing the RNA. And there are a lot more weird and diverse and bespoke cell types in the steering subsystem basically than there are in the learning subsystem. Like the cortical cell types, there's enough to build it seems like there's enough to build a learning algorithm up there and specify some hyperparameters. And in the in this steering subsystem, there's like a gazillion, you know, thousands of really weird cells, which might be like the one for the spider flinch reflex and the one for I'm about to taste salt in the world. &gt;&gt; So why would um each reward function need a different cell type? &gt;&gt; Well, so this is where you get innately wired circuits, right? So in the in the learning algorithm part in this in the learning learning subsystem, um you set up specify the initial architecture, you specify a learning algorithm. all all the all the all the juice is is happening through plasticity of the synapses, changes of the synapses within that big network. But it's kind of like a relatively repeating architecture um how it's initialized. It's just like um the amount of Python code needed to make, you know, a eight layer transformer is not that different from one to make a three layer transformer, right? You're just replicating. &gt;&gt; Yeah. Whereas all this Python code for the reward function, you know, if superior clickless sees something that's skittering and land, you know, you're feeling goosebumps on your skin or whatever, then trigger spider reflex. That's just a bunch of like bespoke species specific &gt;&gt; uh situation specific crap that know the cortex doesn't know about spiders. It just knows about layers and &gt;&gt; Right. But you're saying that the only way to have this like write this reward function &gt;&gt; is to have a special cell type. &gt;&gt; Yeah. Yeah. Well, I think so. I think you either have to have a special cell types or you have to somehow some somehow otherwise get special wiring rules that evolution can say this neuron needs to wire to this neuron without any learning. And the way that that is most likely to happen, I think, is that those cells express like different receptors and proteins that say, okay, when this one comes in contact with this one, let's form a synapse. &gt;&gt; Um, so it's genetic wiring. Um, yeah, and those need cell types to do it. Yeah, I'm sure this would make a lot more sense if I knew 101 neuroscience, but like it seems like there's still a lot of complexity or generality rather in the steering system. So the steering system has its own visual uh system that's separate from the visual cortex. &gt;&gt; Yeah. [snorts] &gt;&gt; Different features still need to plug into that vision system in the so like the spider thing needs to plug into it and also the um the uh love thing needs to plug into it etc etc. So &gt;&gt; it seems complicated like I &gt;&gt; it's still complicated. That's all the more reason why a lot of the genomic, you know, real estate in the genome and in terms of these different cell types and so on would go into wiring up the steering subsystem. &gt;&gt; And can we tell &gt;&gt; pre-wiring it? &gt;&gt; Can we tell how much of the genome is like clearly working? So I guess you could tell how many are relevant to the producing the RNA that that manifest or the epigenetics that manifest in different cell types in the brain, right? &gt;&gt; Yeah. This is what the cell types helps you get at it. And I don't think I don't think it's exactly like oh this percent of the genome is doing this but you could say okay in these all these steering subs subtypes you know how many different genes are involved in sort of specifying which is which and how they wire um and how much genomic real estate do those genes take up um versus the ones that specify you know visual cortex versus audio auditory cortex you kind of just reusing the same genes to do the same thing twice whereas the spider reflex hooking up yes you're right they have to they have to build a vision system they have to build some auditory systems and touch systems and navigation type systems. So, you know, even feeding into the hippocampus and stuff like that, there's head direction cells. Even the fly brain, it has innate circuits &gt;&gt; that, you know, figure out its orientation and help it navigate in the world and it uses vision, figure out optical flow of how it's flying and you know, &gt;&gt; uh, how is it how is its flight related to the wind direction. It has all these innate stuff that I think we in the mammal brain, we would all put that in lump that into the steering subsystem. So, there's a lot of work. So all the genes basically that go into specifying all the things a fly has to do. We're going to have stuff like that too just all in the steering subsystem. &gt;&gt; But do we do we have some estimate of like here's how many nucleotides here how many megabases it takes to &gt;&gt; I I don't know. I mean but but but um I mean I think you might be able to talk to biologists about this you know to to some degree because you can say well we just have a ton in common. I mean we have a lot in common with yeast &gt;&gt; from a genes perspective. used to still use as a model &gt;&gt; for you know some amount of drug development and stuff like that in biology. &gt;&gt; And so so much of the genome is just going towards you have a cell at all. It can recycle waste. It can get energy. It can replicate. &gt;&gt; Um and then then you what we have in common with a mouse. And so we we do know at some level that you know the difference between us and a chimpanzeee or something and that includes the social instincts and the more advanced you know differences in cortex and so on. Um it's it's a it's a tiny number of genes that go into these additional amount of making the eight layer transformer instead of the six layer transformer or tweaking that reward function or &gt;&gt; this would help explain why the homminid brain exploded in size so fast which is presumably like tell me this is correct but under the story we um social learning or some other thing increased the ability to learn from the environment. It like increased our sample efficiency, right? Instead of having to go and kill the boar yourself and figure out like how to do that, you can just be like, &gt;&gt; uh, the elder told me this is how you make a spear and then now it increases the incentive to have a bigger cortex which can like learn these things. Yes. &gt;&gt; And that can be done with a relatively few genes because it's really it's really replicating what the mouse already has. It's making more of it. It's maybe not exactly the same &gt;&gt; and there may be tweaks, but it's like from a perspective, you don't have to reinvent, &gt;&gt; right? &gt;&gt; All this stuff, right? So then um how far back in the history of of the evolution of the brain does the cortex go back and is the idea that like the cortex has always figured out this omnidirectional inference thing that that's been a solve problem for a long time and then the big unlock with primates is this we got the reward function which increased the returns to having omniirectional inference &gt;&gt; or is the cortex is a the omniirectional inference also something that took a while to unlock &gt;&gt; I'm not sure that there's agreement about that I think there might be specific questions about language you know are there tweaks to be able, you know, whether that's through auditory and memory, some combination of auditory memory regions. There may also be like um macro wiring, right, of like you need to wire auditory regions into memory regions or something like that and into some of these social instincts to get &gt;&gt; I see &gt;&gt; language for example to happen. So there might be but that might be also a small number of gene changes yeah &gt;&gt; to be able to say oh I just need from my temporal lobe over here going over to the auditory cortex or something right and there is some evidence for the you know the bronco's area wariki's area they're connected with these hypocampus and so on &gt;&gt; and so prefrontal cortex so there's like some small number of genes maybe for like enabling humans to really properly do language that could be a big one &gt;&gt; but yeah I mean I think that Is it that something changed about the cortex and it became possible to do these things or is there was that potential was already there but there wasn't the incentive to expand that capability and then use it wired it to these social instincts and and use it more. &gt;&gt; Um I mean I would lean somewhat toward the latter. I mean I think a mouse I has a lot of similarity in terms of cortex as a human. Right. Although there's that uh the Sen Hercil work of the &gt;&gt; the um &gt;&gt; the the number of neurons scales better with weight with primate brains than it does with rodent brains. Right. So &gt;&gt; yeah, &gt;&gt; does that suggest that there actually was some improvement in the scalability of the cortex? &gt;&gt; Maybe. Maybe. I'm not I'm not super deep on this. There may there may have been Yeah. changes in architecture, changes in the folding, changes in neuron properties and stuff that that somehow slightly tweak this, but they're still a scaling. That's right. Right. Either way, right? &gt;&gt; Um, and so I'm not saying there aren't something special about humans in the architecture of the learning subsystem at all. Um but yeah, I mean I think it's pretty widely thought that this is expanded but then the question is okay well how does that how does that fit in also with the steering subsystem changes and the instincts that make use of this and allow you to bootstrap &gt;&gt; using this effectively. Um but I mean just to say a few other things I mean so even the fly brain has some amount of for example even even very far back um I mean I think you've read this this great book the brief history of intelligence right I think this is a really good book lots of AI researchers think this is a really good book it seems like &gt;&gt; um yeah you have some amount of learning going back um all the way uh to anything that has a brain basically um you have something kind of like primitive reinforcement learning at least um going back at least to like vertebrates like imagine like a zebra fish like a &gt;&gt; um then there's kind of these other branches birds maybe kind of reinvented something kind of cortex-l like but it doesn't have the six layers &gt;&gt; um but they have something a little bit cortex-like um so that that some of those things um after reptiles in some sense birds and mammals both kind of made a somewhat cortex-like but differently organized thing but even a fly brain has like associative learning centers that um actually do things that maybe look a little bit like this like thought assessor concept from from beer where there's like a specific dopamine signal to train specific subgroups of neurons in the fly mushroom body to associate different sensory information with am I going to get food now or am I going to get hurt now? &gt;&gt; Yeah, &gt;&gt; brief tangent. I remember reading in uh one blog post that Baron Miller wrote that the parts of the cortex which are associated with audio and vision have skilled disproportionately between other primates and humans whereas the parts associated say with odor have not. And I remember him saying something like this is explained by that kind of data having worse scaling law properties. But I think the and maybe he meant this, but another interpretation of actually what's happening there is that these social reward functions that are built into the steering subsystem needed to make use more of being able to see your elders and see what the visual cues are and hear what they're saying. Yeah. &gt;&gt; And in order to make a sense of these cues which guide learning, you needed to activate these um &gt;&gt; activate the vision and audio more than &gt;&gt; I mean there's all this stuff I feel like has come up in in your your shows before actually, but like &gt;&gt; even like the design of the human eye where you have like the pupil and the white and everything like we are designed to be able to establish relationships based on joint eye contact and and maybe this came up in the sudden episode. I can't remember but &gt;&gt; um yeah we we have to bootstrap to the point where we can detect eye contact and where we can communicate by language right and that's like what the the first [clears throat] couple years of life are are trying to do yeah &gt;&gt; okay uh I want to ask you about RL so um currently the way these elements are trained you know they are um if if they solve the unit test or solve a math problem that whole trajectory every token in that trajectory is up weighted and what's going on with humans is there are there different types of model based versus model free that are happening in different parts of the brain. &gt;&gt; Yeah, I mean this is this is another one of these things. I mean again all my answers to these questions any specific thing I say it's all just kind of like directionally this is we can kind of explore around this. I find this interesting. Maybe I feel like the literature points in these directions in some very broad way. What I actually want to do is like go and map the entire mouse brain and like figure this out comprehensively and like make neuroscience the ground truth science. So I don't know basically. [laughter] Um but uh but yeah I mean there so first of all I mean I think with Ilia on the podcast I mean he was like it's weird that you don't use value functions right you use like the most dumbest form of RL bas and of course there are these people are incredibly smart and they're optimizing for how to do it on GPUs and it's really incredible &gt;&gt; what they're achieving but like conceptually it's a really dumb form of RL even compared to like what was being done in like 10 years ago right like even uh you know the Atari game playing stuff right was using like Q-learning which is basically like It's a kind of temporal difference learning, right? And the temporal difference learning basically means you have some kind of a value function of like what action I choose now doesn't just tell me literally what happens immediately after this. It tells me like what is the long run consequence of that for my expected you know total reward or something like that. &gt;&gt; Um and so you would have value functions like the fact that we don't have like value functions at all is like in the LLMs is like it's crazy. I mean, I I think I think because Ilia said it, I I can say it. I know, you know, one one 100th of what he does about AI, but like it's kind of crazy that this is working. &gt;&gt; Yeah. [laughter] &gt;&gt; Um but, uh yeah, I mean, in terms of the brain, um well, so I think there are some parts of the brain that are thought to do something that's very much like model free RL that are parts of the basil ganglia. um sort of stratum and basil ganglia they have like a a certain finite like it is thought that they have a certain like finite relatively small action space and the types of actions they could take first of all might be like tell the spinal cord or tell the brain stem and spinal cord to do this motor action. Yes. No. Um or it might be more complicated cognitive type actions like tell the phalamus to allow this part of the cortex to talk to this other part or release the memory that's in the hypocampus and start a new one or something. Right? that there's but there's some finite set of actions that kind of come out of the basil ganglia and that it's just a very simple RL. So there are probably parts of other brains in our brain that are just like doing very simple naive type RL algorithms. um layer one thing on top of that is that some of the major work in neuroscience like Peter Dian's work and a bunch bunch of work that is part of why I think deep mind did the temporal difference learning stuff in the first place um is they were very interested in neuroscience um and there's a lot of neuroscience evidence that the dopamine is giving this reward prediction error signal um rather than just reward yes no you know a gazillion time steps in the future it's a prediction error um and that's consistent with like learning these value functions. Um, so there's that and then there's maybe like higher order stuff. So we have these cortex making this world model. Well, one of the things the cortex world model can contain is a model of when you do and don't get rewards, right? Again, it's predicting what the steering subsystem will do. It could be predicting what the bas ganglia will do. &gt;&gt; [snorts] &gt;&gt; And so you have a model in your cortex that has more generalization and more concepts and all this stuff that says okay these types of plans these types of actions will lead in these types of circumstances to reward. So I have a model of my reward. Um some people also think that you can go the other way and so this is part of the inference picture. There's this idea of RL as inference. Um you could say well conditional on my having a high reward sample a plan that I would have had to get there. That's inference of the plan part from the reward part. I'm clamping the reward as high and inferring &gt;&gt; the plan sampling from plans that could lead to that. &gt;&gt; Um and so if you have this very general protocol thing it can just do if you have this like general very general model based system and the model among other things includes plans and rewards then you just get it for free. basically. &gt;&gt; So like in neural network parlament there's a value head associated to the the the omniirectional inference that's happening in the or there's a value input. Um yeah &gt;&gt; oh &gt;&gt; yeah and it and it it can predict one of one of the one of the almost sensory variables it can predict is &gt;&gt; is what rewards it's going to get. &gt;&gt; Yeah. But by but speaking of this thing about amortizing things um yeah obviously value is like amortized rollouts &gt;&gt; of looking up reward. &gt;&gt; Yeah something like that. Yeah. Yeah. It's like a statistical average or prediction of it. Yeah. &gt;&gt; Right. Tangential thought. uh you know Joe Joe Henrik and others have this idea that the way human societies have learned to do things is just like how do you figure out that you know this kind of bean which actually just almost always poisons you is edible if you do this 10step incredibly complicated process any one of which if you fail at the bean will be poisonous. How do you figure out how to hunt this seal in this particular way with this like particular weapon at this particular time of the year etc. Um there's no way but uh just like trying [&nbsp;__&nbsp;] over generations &gt;&gt; and it strikes me this is actually very much like model free RL happening at like a civilizational level. Um no not exactly &gt;&gt; because evolution is the simplest algorithm in some sense right and if we believe that all this can come from evolution like the outer loop can be like extremely not foresighted and yeah &gt;&gt; right um that that's interesting just like uh hierarchies of evolution model for a culture uh evolution model for &gt;&gt; so what does that tell you maybe that simple algorithms can just get you anything if you do it enough or yeah I don't know so &gt;&gt; but yeah so you you have like maybe this yeah evolution model free basil ganglia model free cortex model based &gt;&gt; culture uh model free potentially um I mean there's like you pay attention to your elders or whatever so there's &gt;&gt; maybe there's like group selection or whatever of these things is like more model free yeah &gt;&gt; but now I think culture well it stores some of the model &gt;&gt; yeah right &gt;&gt; so let's say you want to train an agent to help you with something like processing loan applications training an agent to do this requires more than just giving the model access to the right tools things like browsers and PDF readers and risk models. There's this level of task and knowledge that you can only get by actually working in an industry. For example, certain loan applications will pass every single automated check despite being super risky. Every single individual part of the application might look safe, but experienced underwriters know to compare across documents to find subtle patterns that signal risk. Labelbox has experts like this in whatever domain you're focused on, [music] and they will set up highly realistic training environments that include whatever subtle nuances and watch outs you need to look out for. Beyond just building the environment itself, Labelbox provides all the scaffolding you need to capture training data for your agent. They give you the tools to grade agent performance and capture the video of each session and to reset the entire environment to a clean state between every episode. So, whatever domain you're working in, Labelbox can help you train reliable realworld agents. Learn more at labelbox.com/thorcash. Stepping back, how um is it a disadvantage or an advantage for humans that we get to use biological hardware in comparison to computers as exist now? So let by what I mean by this question is like if there's the algorithm would the algorithm just qualitatively perform much worse or much better if um inscribed in the hardware today and the reason to think it might like here's what I mean like you know obviously the brain has had to make a bunch of trade-offs which are not relevant to computing hardware it has to be much more energetically efficient maybe as a result it has to learn run on slower speeds so that there can be smaller voltage gap and so the brain runs at 200 hertz um and has to like run on 20 watts on the other And may you know with like robotics we've clearly experienced that fingers are way more nimble than we can make motors so far. And so maybe there's something in the brain that is the equivalent of like cognitive uh dexterity which is like maybe due to the fact that we can do unstructured sparsity we can colllocate the memory and the compute. &gt;&gt; Yes. &gt;&gt; Where does this all out? Are you like [&nbsp;__&nbsp;] we would be so much smarter if we didn't have to deal with these brains. Or are you like oh &gt;&gt; I mean I think in the end we will get the best of both worlds somehow right. I think I think an obvious downside of the brain is it cannot be copied. Yeah. you don't have you know external readr access to every neuron and synapse &gt;&gt; whereas you do I can just edit something in the weight matrix right &gt;&gt; you know in python or whatever &gt;&gt; uh you know and and load that up and copy that um in principle right um so the fact that it can't be copied and kind of random accessed is like very annoying but otherwise maybe these are it like has a lot of advantages so or it also tells you that you want to like somehow do the code design of the algorithm and uh it maybe It even doesn't change it that much from all what we discussed, but you want to somehow do this code design. So, um yeah, how do you do it with really slow low voltage switches? That's going to be really important for the energy consumption, the collocating memory and compute. So, like I I think that probably just like hardware companies will try to colllocate memory and compute. They will try to use lower voltages, allow some stoastic stuff. There are some people that think that this like all this probabilistic stuff that we were talking about. Oh, oh, it's actually energy based models and so on is doing lot it is doing lots of sampling. It's not just amortizing everything that the neurons are also very natural for that because they're naturally stoastic. &gt;&gt; And so you don't have to do a random number generator and a bunch of Python code basically to generate a sample. &gt;&gt; The neuron just generates samples and it can tune what the different probabilities are. &gt;&gt; Yeah. and so and like learn learn those tunings and so &gt;&gt; it could be that it's very co-designed with like some kind of inference method or something. &gt;&gt; Yeah, &gt;&gt; it'd be hilarious. I mean the message for these interviews like you know all these people that folks make fun of on Twitter you know Yan Lool Yan Lun and Beth Jos and whatever they're like no like yeah maybe I don't know &gt;&gt; that is actually that is actually one read of granted you know I I haven't really worked on AI at all since LM's you know took off so I'm just like out of the loop but &gt;&gt; um I'm surprised and I'm I I I think it's amazing how the scaling is is working and everything but yeah I think Yan Lun and Bef Jezos are kind of on to something about the about the pro models or at least possibly and in fact that's what &gt;&gt; you know all the neuroscientists and all the AI people thought like until 2021 or something right &gt;&gt; so there's a bunch of cellular stuff happening in the brain that is not just about neuronto neuron synaptic connections how much of that is functionally doing more work than the synapses themselves are doing versus it's just a bunch of kudge that you have to do in order to make the synaptic thing work. So the way you need to you know with a digital mind you can nudge the synapse sorry the parameter extremely easily &gt;&gt; but with a cell to modulate a synapse according to the gradient signal it just takes so all of this crazy machinery. So like is it actually doing more than it takes extremely little code to do? So, I don't know. But I'm I'm not a believer in the like radical like, oh, actually memory is not synapses mostly or like learning is mostly genetic changes or something like that. &gt;&gt; I think it would just make a lot of sense. I think you put it really well for it to be more like the second thing you said like let's say you want to do weight normalization across all the weights coming out of your neuron, right? Or into your neuron. Well, you probably have to like somehow tell the nucleus about this of the cell and then have that kind of send everything back out to the synapsis or something, right? And so there's going to be a lot of cellular changes, right? Or let's say that, &gt;&gt; you know, you just had a lot of plasticity and like you're part of this memory and now that's got consolidated into the cortex or whatever [snorts] and now we want to reuse you as like a new one that can learn again. It's going to be a ton of cellular changes. So there's going to be tons of stuff happening in the cell, but algorithmically it's not really adding something beyond these algorithms, right? It's just implementing something that in a digital computer is very easy for us to go and just find the weights and change them. &gt;&gt; Um, and it is a cell. It just literally has to do all this with molecular machines itself. &gt;&gt; Um, without any central controller, right? It's kind of incredible. &gt;&gt; Um, there are some things that cells do I think that that seem like more convincing. So in the cerebellum so one of the things the cerebellum has to do is like predict over time like predict what is the time delay you know let's say that um you know I see a flash and then you know some number of milliseconds later I'm going to get like a puff of air in my eyelid or something right uh the cerebellum can be very good at predicting what's the timing between the flash and the air puff so that now your eye will just like close automatically like the cerebellum is like involved in that type of reflex like learned reflex Um, and there are some cells in the cerebellum where it seems like the cell body is playing a role in storing that time constant, changing that time constant of delay versus that all being somehow done with like I'm going to make a longer ring of synapses to make that delay longer. It's like no, the cell body will just like store that time delay for you. Um, so there are some examples, but I'm not a believer like out of the box in like essentially this theory that like what's happening is changes and connections between neurons. &gt;&gt; Yeah. &gt;&gt; And that's like the main algorithmic thing that's going on that like I I think that's a very good reason to to still believe that it's that rather than some like crazy cellular stuff. &gt;&gt; Yeah. Going back to this whole perspective of like our our intelligence is not just this omniirectional inference thing that builds a world model but really this system that teaches us what to pay attention to what are the important salient factors to learn from etc. I I want to see if there's some intuition we can drive from this but what different kinds of intelligences might be like. So it seems like AGI or superhuman intelligence should still have this um uh uh like ability to learn a world model that's quite general, &gt;&gt; but then it might um be incentivized to pay attention to different things &gt;&gt; that are relevant for &gt;&gt; what you know the the modern post singularity environment. How different should we expect different intelligences to be basically? &gt;&gt; Yeah. I mean, I think one way of this question is like, is it actually possible to like make the paperclip maximizer or whatever, right? If you make if you try to make the paperclip maximizer, does that end up like just not being smart or something like that because it's it was just the only reward function it had was like make paper clips. &gt;&gt; Interesting. Yeah. Yeah. If I channel Steve Burns more, I mean, I think he's very concerned that the the sort of minimum viable things in the steering subsystem that you need to get something smart is way less than the minimum viable set of things you need for it to have human like social instincts and ethics and stuff like that. So, a lot of what you want to know about the steering subsystem is actually the specifics of how you do alignment essentially or what human behavior and social instincts and is versus just what you need for capabilities. is we talked about it in a slightly different way cuz we were sort of saying well in order for humans to like learn socially they need to make eye contact and learn from others but we already know from LLMs right that depending on your starting point you can learn language without that stuff right &gt;&gt; and so yeah and and so I think that it probably is possible to make like super powerful you know modelbased RL you know optimizing systems and stuff like that that don't have most of what we have in the human brain reward functions and as a consequence might want to maximize paper clips and that's a concern. Yeah. &gt;&gt; Right. But but you're pointing out that &gt;&gt; in order to make a competent paperclip maximizer, the kind of thing that can build the spaceships and learn the physics and whatever um &gt;&gt; it needs to have some drives which elicit learning including say curiosity and exploration. &gt;&gt; Yeah. Curiosity in interest in others uh of so interest in social interactions, curiosity. Um &gt;&gt; yeah, but but that that's pretty that's pretty minimal I think. And it and that's true for humans, right? &gt;&gt; But it might be less true for like something that's already pre-trained as an LLM or something, right? And so uh so most of why we want to know the steering subsystem, I think, uh if I'm channeling Steve is alignment reasons. Yeah. &gt;&gt; Right. &gt;&gt; How how confident are we that we even have the right algorithmic conceptual vocabulary to think about what the brain is doing. And what I mean by this is you know there was one big contribution to AI from neuroscience which was this idea of the neuron which like William you know 1950s just like this original contribution but then it seems like a lot of what we've learned afterwards about what the high level algorithm the brain is implementing from the backrop to if there's something analogous to backrop happening in the brain to oh is V1 doing something like CNN's to TD learning and bellman equations um actor critic seems inspired by what is like we come up with some idea like maybe we can make AI neural networks work this way and then [clears throat] we notice that something in the brain also works that way. So why not think there's more things like this where &gt;&gt; there may be. Yeah. I think the reason that I'm not I think that we might be on to something is that like the AI we're making based on these ideas are working surprisingly well. There's also a bunch of like just empirical stuff like like convolutional neural nets and variants of convolutional neural nets. Um I'm not for sure what the absolute latest latest but compared to other like models in computational neuroscience of like what the visual system is doing are just like more predictive right? Right. So you can just like score &gt;&gt; um even like pre-trained on like cat pictures and stuff CNN's what is the representational similarity that they have on some arbitrary other image versus you know compared to the brain activations um measured in different ways. Um Jim De Carlo's lab has like brain score and like the AI models actually like there there seems to be some relevance there in terms of like even like neuroscientists don't necessarily have something better than that. So yes, I mean that's just kind of recapitulating what you're saying is that like the best computational neuroscience theories we have seem to have been like invented &gt;&gt; largely as a result of AI models um and like find things that work and so find back prop works and then say can we approximate back prop with cortical circuits or something and there's there's kind of been things like that. Now some people totally disagree with this right. Um so like Yuri Buzzaki is a neuroscientist who has a book called the brain from inside out where he basically says like all our psychology concepts like AI concepts all the stuff is just like made up stuff. We actually have to do is like figure out what is the actual set of primitives that like the brain actually uses and our vocabulary is not going to be adequate to that. We have to start with the brain and make new vocabulary rather than saying back prop and then try to apply that to the brain or something like that. And you know, he studies a lot of like oscillations and stuff in the brain as opposed to individual neurons and what they do. And &gt;&gt; you know, I don't know. I I think that there's a case to be made for that. And from a kind of research program design perspective, I think there's like one thing we should be trying to do is just like simulate a tiny worm or a tiny zebra fish. um like from almost like as biohysical or like as as bottom up as possible like get pento molecules activity and like just study it as a physical dynamical system and like look what it does. Um but I don't know I mean just when I like it just feels like the AI is really good fodder for computational neuroscience. Like those might actually be pretty good models. We should look at that. Um, so I I'm not a person who thinks that I I I think I I both think that there should be a part of the research portfolio that is like totally bottom up and not trying to apply our vocabulary that we learn from AI onto these systems and that there should be a another big part of this that's kind of trying to reverse engineer it using that vocabulary or variance of that vocabulary. um and that we should just be pursuing both and and my guess is that the reverse engineering one is actually gonna like kind of workish or something like we do see things like TD learning which you know sudden also invented right &gt;&gt; separately right &gt;&gt; that must be a crazy feeling to just like &gt;&gt; yeah it's crazy this like equation I wrote down is like in the brain &gt;&gt; it seems like the dopamine is like doing some of that yeah &gt;&gt; so let me ask you about this uh you know you guys are fighting different groups that are trying to &gt;&gt; figure out what's up in the brain. If we had a perfect representation, however you define it, of the brain, why think it would actually let us figure out the answer to these questions? We have neural networks which are way more interpretable not just because we understand what's in the weight matrices, but because there are weight matrices. There are these boxes with numbers in them, right? &gt;&gt; And even then, we can tell very basic things. we can kind of see circuits for uh very basic pattern matching of following one token with another. I I feel like we we don't really have an explanation of why LLM are intelligent just because they're somewhat I would somewhat dispute. I think we have some architectural we have some description of what the LLM is like fundamentally doing and what that's doing is that I have an architecture and I have a learning rule and I have hyperparameters and I have initialization and I have training data &gt;&gt; but that those are things we learned from because we built them not because we interpreted them from seeing the waves which is the the thing to connect to them is like seeing the weights &gt;&gt; what I think we should do is we should describe the brain more in that language of things like architectures learning rules initializations rather than trying to find the golden gate bridge circuit and saying exactly how does this neuron actually, you know, that's going to be some incredibly complicated learned pattern. &gt;&gt; Um, yeah, Conor Cording and Tim Lilly crap have this paper from a while ago, maybe five years ago called what does it mean to understand a neural network or what would it mean to understand a neural network? Um, and what they say is, yeah, basically that like you can imagine you train a neural network to like compute the digits of pi or something. Well, like some crazy, you know, it's like it's like this crazy pattern. And then you also train that thing to like predict the most complicated thing you find, predict stock prices, basically predict the really complex systems, right? Computation, you know, computationally complete systems. I could predict I could train a neural network to do cellular automa or whatever crazy thing. It's like we're never going to be able to fully capture that with interpretability. I think it's just going to just be doing really complicated computations internally, but we can still say that the way it got that way is that it had an architecture and we gave it this training data and it had this loss function. And so I want to describe the brain in the same way. And I think that this framework that I've been kind of laying out is like we need to understand the cortex and how it embodies learning algorithm. I don't need to understand how it computes Golden Gate Bridge. &gt;&gt; If you if you can see all the neurons if you have the conneto, why does that teach you what the learning algorithm is? &gt;&gt; Well, I guess there are a couple different views of it. So it depends on there's different parts of this portfolio. So on the totally bottom up we have to simulate everything portfolio. It kind of just doesn't you have to just like see what are the you have to make a simulation of the zebra fish brain or something and [snorts] then you like see what are the like emergent dynamics in this and you come up with new names and new concepts and all that. That's like that's like the most extreme bottomup neuroscience view. Um but even there the conneto is like really important for doing that bottom biohysical or bottomup simulation. Um but on the other hand you can say well what if we can actually apply some ideas from AI we basically need to figure out is it an energy based model or is it you know an amortized you know VAE type model you know is it doing back prop or is it doing something else um are the learning rules local or global I mean if we have some repertoire of possible ideas about this can we just think of the conneto as a huge number of additional constraints that will help to refine to ultimately have a consistent picture of that. I think about this for the the steering subsystem stuff too. Just very basic things about it. How many different types of dopamine signal or of steering subsystem signal or thought assessor or so on. How many different types of what broad categories are there? Like even this very basic information that there's more cell types in the hypothalamus than there are in the cortex. Like that's new information, right? About how much structure is built there versus somewhere else. Yeah. How many different dopamine neurons are there? is the wiring between prefrontal and auditory the same as the wiring between prefrontal and visual. You know, it's like uh the most basic things we don't know. And the problem is learning even the most basic things by a series of bespoke experiments takes an incredibly long time whereas just learning all that at once by getting a conneto is just like way more efficient. &gt;&gt; What is the timeline on this? Because presumably the idea of this is to um well first inform the development of AI. You want to be able to figure out how we do the um how we get AIS to want to care about what other people think of its internal thought pattern. But interpret researchers are making progress on this question just by inspecting you know normal neural networks. There must be some feature &gt;&gt; they're make. You can do interp on LLMs that exist. &gt;&gt; Yeah. &gt;&gt; You can't do interp on &gt;&gt; a hypothetical model based reinforcement algorithm like the brain that we will eventually converge to when we do AGI. &gt;&gt; Fair. But um yeah, you know what what what timelines on AI do you need for this research to be practical and relevant? &gt;&gt; I think it's fair to say it's not super practical and relevant if you're in like AI 2077 scenario. &gt;&gt; Yeah. &gt;&gt; You know, and so like what science I'm doing now is not going to affect the science of like 10 years from now because what's going to affect the science of 10 years from now is the outcome of this like AI 2027 scenario, right? It kind of doesn't matter that much. probably if I have the conneto maybe it slightly tweaks certain things but um but I think there there's a lot of reasons to think maybe that we will get a lot out of this paradigm but then the real thing the thing that is like the the trans the the like single event that is like transformative for the entire future or something type event is still like you know more than 5 years away or something is that because the like &gt;&gt; we haven't captured predominant directional inference. We haven't figured out the right ways to get a mind to pay attention to things in a way that makes sense. &gt;&gt; I mean, I would take the entirety of your like collective podcast with everyone as like showing like the distribution of these things, right? I don't know, right? &gt;&gt; Um I what was Carpathy's timeline, right? You know, what's Deis' timeline, right? So these not everybody has a three-year timeline and and so I think &gt;&gt; there's different reasons and I'm curious reasons. What are mine? I don't know. I'm just watching your podcast. I'm trying I'm trying to understand the distribution. I don't have a super strong claim that LLM can't do it. Um, &gt;&gt; but is it corros like the data efficiency or is it the &gt;&gt; I think part of it is just it is weirdly different than all this brain stuff. &gt;&gt; Yeah. Yeah. &gt;&gt; And so intuitively it's just weirdly different than all this brain stuff. And I'm kind of waiting for like the thing that starts to look more like I think if Alpha Zero and model based RL and all these other things that were being worked on 10 years ago had been giving us the GPT5 type capabilities then I would be like oh wow we're both in the right paradigm and seeing the results &gt;&gt; right &gt;&gt; a priori. So my model, my prior and my data are agreeing, &gt;&gt; right? And now it's like I don't know what exactly my data is. Looks pretty good, but my prior is &gt;&gt; sort of weird. So yeah, so so I don't have a super strong opinion on it. So I think there's a possibility that essentially all other scientific research that is being done is like not is somehow obviated, but I don't put a huge amount of probability on that. I think my timelines might be more in the like yeah 10 yearish range. And if that's the case, I mean, I think there yeah, there is probably a difference of pun world where we have connetos on hard drives and we have understanding of steering subsystem architecture. We've compared the the you know even the most basic properties of what are the reward functions, cost function architecture, etc. of you know mouse versus a shrew versus a small primate etc. &gt;&gt; Is this practical in [clears throat] 10 years? &gt;&gt; Uh I think it has to be a really big push. Um &gt;&gt; like how much funding how does it compare to where we are now? It's like billion low billions dollar scale funding in a very concerted way I would say. &gt;&gt; How much is on it now? &gt;&gt; Um well so so if I just talk about some of the specific things we have going so with connecttoics. So &gt;&gt; uh so E11 bio is kind of like the the our main thing on kontoics. &gt;&gt; Um they are basically trying to make the technology of contoic brain mapping um several orders of magnitude cheaper. So the welcome trust put out a report uh a year or two ago that basically said to get one mouse brain the first mouse brain conneto would be like several billion dollars you know billions of dollars project. Um well E1 technology and sort of the suite of efforts in the field also are trying to get like a single mouse conneto down to like low tens of millions of dollars. Okay. So that's a mammal brain right now. A human brain is about a thousand times bigger. So if a mouse brain you can get to 10 million or 20 million 30 million um with technology you know if you just naively scale that okay human brain is now still billions of dollars to just one do do one human brain can you go beyond that can you get a human brain for like less than a billion but I'm not sure you need every neuron in a human brain I think we want to for example do an entire mouse brain and a human steering subsystem and the the entire brains of several different mammals with different social instincts. Um and so I think that that with a bunch of technology push and a bunch of concerted effort can be done in the real significant progress if it's focused effort can be done in the kind of hundreds of millions to low billions. &gt;&gt; What is the definition of a contome? Is it um presumably it's not a bottomup bioysics model? So is it just that if if it can estimate the input output of a brain but like what is what is the level abstraction? So you can give different definitions and one of the things that's cool about [snorts] so the the kind of standard approach to connecttoics uses the electron microscope and very very thin slices of brain tissue and it's basically labeling the cell membranes are going to show up scatter electrons a lot and everything else is going to scatter electrons less but you don't see a lot of details of the molecules which types of synapses different synapses have different molecular combinations and properties. E1 um and some other research in the field has switched to an optical microscope paradigm. With optical the photons don't damage the tissue. So you can kind of wash it and look at fragile gentle molecules. Um so so with E1 approach you can get a quote unquote molecularly annotated conneto. So that's not just who is connected to who by some kind of syninnapse but what are the molecules that are present at the syninnapse. What type of cell is that? So a molecularly annotated conneto that's not exactly the same as having the synaptic weights. Um that's not exactly the same as being able to simulate the neurons and say what's the functional out functional consequence of having these molecules and connections. Um but you can also do some amount of activity mapping and try to correlate structure to function. Um yeah so &gt;&gt; interesting &gt;&gt; train an ML model to basically predict the activity from the conneto. What are the lessons to be taken away from um the human genome project? Because one way you could look at it is that it was actually a mistake and you shouldn't have spent whatever billions of dollars getting one &gt;&gt; genome mapped rather you should have just invested in technologies which have and now now allows to map genomes for hundreds of dollars. &gt;&gt; Yeah. Well, yeah. So George Church was my was my PhD adviser and and basically &gt;&gt; uh yeah I mean what he's pointed out is that yeah it was three billion or something you roughly $1 per base pair for the first genome and then &gt;&gt; the National Human Genome Research Institute basically structured the funding process right and they got a bunch of companies competing to lower the cost. Um and then the cost dropped like a million fold in 10 years. Um, and because they changed the paradigm from uh kind of macroscopic kind of chemical techniques to these individual DNA molecules make a little cluster of DNA molecules on the microscope and you would see um just a few DNA molecules at a time on each pixel of the camera would basically give you a different um in parallel looking at different fragments of DNA. So you parallelize the thing by like millions fold and that's what reduced the cost by millions fold. and um and yeah so so I mean essentially uh with switching from electron microscopy to optical connetoics potentially even future types of connetoics technology we think there should be similar patterns that's why E1 with the focus research organization uh started with technology development rather than starting with saying we're going to do a human brain or something let's just brute force it we said let's get the cost down with new technology but then you still it's still a big thing even with new next generation technology you still need to spend hundreds of millions hands- on data collection. &gt;&gt; Yeah. &gt;&gt; Is this going to be funded with philanthropy, by governments, by investors? &gt;&gt; This is very TBD and very much evolving in some sense as we speak. Um I'm hearing some rumors going around of connetoxics related companies potentially forming. But so so far E1 has been philanthropy. Um the National Science Foundation just put out this call for for tech labs, which is basically somewhat of it is kind of fro inspired or or related. Um, I think you could have a tech lab uh for actually going and mapping the mouse brain with this and that would be sort of philanthropy plus government still in a nonprofit kind of open source framework. Um, but can uh can companies accelerate that? Can you credibly link connetoics to AI in the context of a company and get investment for that? It's like possible. I mean the cost of training these AI is increasing so much if you could like tell some story of like &gt;&gt; not only are we going to figure out some safety thing but in fact we will &gt;&gt; um &gt;&gt; once we do that we'll also be able to tell you how AI works. I mean all these you should like go to these AI labs and just be like give me 1/100th of your projected budget in 2030. &gt;&gt; I sort of tried a little bit like like seven or eight years ago and there was not a lot of interest and maybe now there there would be. Um but yeah, I mean I think all the things that we've been talking about like I think it's really fun to talk about but it's like ultimately speculation. What is the actual reason for the energy efficiency of the of the brain for example, right? Is it doing real inference or immortiz inference or something else like the this is all going to be alerable by neuroscience. It's going to be hard but it's actually answerable. Um, and so if you can only do that for low billions of dollars or something to really comprehensively solve that, it seems to me in the grand scheme of trillions of dollars of GPUs and stuff, it actually makes sense to do that investment. But &gt;&gt; and and I think investors also just there's been many labs that have been launched in the last year where they're raising on the valuation of billions for things which are quite credible but are not like &gt;&gt; our arr next quarter is going to be whatever. It's like we're we're going to discover materials and dot dot dot, right? &gt;&gt; Yes. Yes, moonshot startups or billion dollar billionaire back startups, moonshot startups. I see as kind of on a continuum with froze. &gt;&gt; Um, froze are a way of channeling philanthropic support and ensuring that it's open source, public benefit, various other things that that may be properties of a given fro. Um, but yes, billionaire back startups um if they can target the right science, the exact right science. I think there's a lot of ways to do moonshot neuroscience companies that would never get you the conneto. like, "Oh, we're going to upload the brain or something and but never actually get the the mouse conneto or something." These fundamental things that you need to get to to ground truth the science. Um, there are lots of ways to have a moonshot company kind of go wrong and not do the actual science, but there also may be ways to have companies or or big corporate labs get involved and actually do it correctly. Yeah. M this uh this brings to mind an idea that you had in a lecture you gave 5 years ago about &gt;&gt; Yeah. Do you want to explain behavior cloning on &gt;&gt; right? Yeah. I mean actually this is funny because I think that the first time I saw this idea it was I think it actually might have been in a blog post by Gor. &gt;&gt; Oh &gt;&gt; there's always there's always a Gor blog post &gt;&gt; and there are now academic research efforts and some amount of emerging company type efforts to try to do this. So um yeah, so normally like let's say I'm training an image classifier or something like that. I show it pictures of cats and dogs or whatever and they have the label cat or dog and I have a neural net supposed to predict the label cat or dog or something like that. Um that is a limited amount of information per label that you're putting in. It's just cat or dog. What if I also had predict what is my neural activity pattern when I see a cat or when I see a dog and all the other things. Um, if you add that as like an auxiliary loss function or an auxiliary prediction task, does that sculpt the network to know the information that humans know about cats and dogs? um and to represent it in a way that's consistent with how the brain represents it and the kind of rep representational kind of dimensions or geometry of of of how the brain represents things as opposed to just having these labels. Does that let it generalize better? Does that let it uh have just richer labeling? And of course, that's like that sounds really challenging. It's very easy to to generate lots and lots of labeled cat pictures with, you know, scale AI or whatever can do this. it is harder to generate lots and lots of brain activity patterns that correspond to things that you want to train the AI to do. Um but again this is just a technological limitation of neuroscience. If we if every iPhone was also a brain scanner, you know, you would we would not have this problem and we would be training AI with the brain signals and um it's just the order in which technology is developed is that we got GPUs before we got portable brain scanners or whatever, right? And uh that kind of thing. &gt;&gt; What is the analog? Will you be doing here? is when you distill models, you're still looking at the the final layer of like the the log props across um across &gt;&gt; if you do distillation of one model into another. That is a certain thing you were just trying to copy one model into another. &gt;&gt; Yeah, &gt;&gt; I think that we don't really have a perfect proposal to like distill the brain. I think to distill the brain, you need like a much more complex brain interface. Like maybe you could also do that. You could make surrogate models. Um Andreas Tolius and people like that are doing some amount of neural network surrogate models of brain activity data. Instead of having your visual cortex do the computation, just have the surrogate models. You're basically distilling your visual cortex into a neural network to some degree. &gt;&gt; Um that's a kind of dissolation. This is doing something a little different. This is basically just saying I'm adding an auxiliary. I think of as regularization or I think of it as um adding an auxiliary loss function um that sort of smoothing out the prediction task to also always be consistent with how the brain represents it. &gt;&gt; What exactly &gt;&gt; it might help you things like adversarial examples for example, right? &gt;&gt; So but you're predicting the internal state of the brain. &gt;&gt; Yes. So in so you so in addition to predicting the label the vector of labels like yes cat not dog yes you know no not boat you know &gt;&gt; um one shot vector or whatever of one hot vector of of yes it's cat instead of these gazillion other categories let's say in this simple example &gt;&gt; you're also predicting a vector which is like all these brain signal measurements &gt;&gt; right &gt;&gt; interesting &gt;&gt; and so anyway had this long ago blog post of like oh this is like an intermediate thing this like we talk about whole brain emulation we talk about AGI we talk about brain computer interface we should also be talking about this like brain augmented brain data augmented um uh thing that's trained on all your behavior but is also trained on like predicting some of your neural patterns &gt;&gt; right and you're saying the learning system is already doing this for the steering system. &gt;&gt; Yeah. And our learning system also has predict the steering subsystem as an auxiliary task. Yeah. &gt;&gt; And that helps the steering sub now the steering subsystem can access that predictor and build a cool reward function using it. Yes. &gt;&gt; Okay. separately. You're on the board for of lean, which is this um uh formal uh uh formal math language uh uh that people mathematicians use to prove theorems and so forth. And obviously there's a bunch of conversation right now about math AI automating math. &gt;&gt; What's your take? &gt;&gt; Yeah. Well, I think that there are parts of math that it seems like it's pretty well on track to &gt;&gt; to automate. Um and that has to do with like so so first of all so so lean so lean had been developed for a number of years at Microsoft and other places has become one of the convergent focused research organizations to kind of drive more engineering and focus onto it. So lean is like this language programming language where if you instead of expressing your math proof on pen and paper um you express it in this programming language lean and then at the end if you do that that way um it is a verifiable language so that you can basically click verify and lean will tell you whether the conclusions of your proof actually follow perfectly from your assumptions of your proof. Um so it checks whether the proof is correct automatically. Um but like by itself this is useful for mathematicians collaborating and stuff like that. Like if I'm some amateur mathematician I want to add to a proof you know Terry Tao is not going to like believe my result. Um but if lean says it's correct it's just correct. So it makes it easy for like collaboration to happen. Um but it also makes it easy for correctness of proofs to be an RL signal in very much the RLVR you know it's like a perfect math proofing is now formalized math proofing so formal means it's like expressed in something like lean and verifiable mechanically verifiable um that becomes a perfect RLVR you know task um yeah and I think that that is going to just just keep working it seems like is a couple billion dollar you at least one like billion dollar valuation company Harmonic based on this Alpha Proof is based on this. Um a couple other emerging really interesting companies. Um I think that this problem of like rlvring the crap out of math proving is basically going to work. uh and we will be able to have things that search for proofs um and find them um in the same way that we have alpha go or what have you that can search for you know ways of playing the game of go and with that verifiable signal uh works. So does this like solve math? Um there is still the part that has to do with conjecturing new interesting ideas. There's still the kind of conceptual organization of math of what is interesting. how do you come up with new theorem statements in the first place or even like the very high level breakdown of what strategies you use to do proofs. Um I mean I think this will shift the burden of that so that humans don't have to do a lot of the mechanical parts of math uh validating lemas and proofs and checking if the statement of this in this paper is exactly the same as that paper and stuff like that. It will just that will just work. uh you know if you really think you're we're going to get all these things we've been talking about real AGI it would also be able to make conjectures and you know Benjio has like a paper as more like theorical paper there probably a bunch of other papers emerging about this like is there like a loss function for like good explanations or good conjectures that's like a pretty profound question right um a math a really interesting math proof or statement might be one that kind of compresses lots of information about other you know has lots of implications for lots of other theorems Otherwise, you would have to prove those themsive inference. Here, if you have this theorem, this theorem is correct. And you have short passive inference to all the other ones. And it's a short compact statement. So, it's like a powerful explanation that explains all the rest of math. And like part of what math is doing is like making these compact things that explain the other things. &gt;&gt; It's like they call the moral complexity of this statement or something. &gt;&gt; Yeah. Of generating all the other statements given that you know this one or stuff like that. Or if you add this, how does it affect the comp the complexity of the rest of the kind of network of proofs? So can you like make a loss function that adds oh I want this proof to be a really highly powerful proof. Um I think some people are trying to work on that. So so maybe you can automate the creativity part. Um if you had true AGI it would do everything a human can do. So it would also do the things that the creative mathematicians do. But um but way barring that I think just rlvring the crap out of proofs um well I think that's going to be just a really useful tool for mathematicians. going to accelerate math a lot and change it a lot, but not necessarily immediately change everything about it. Will we get, you know, mechanical proof of the remon hypothesis or something like that or things like that? Maybe. I don't know. I don't know enough details of how hard these things are to search for. I'm not sure anyone can fully predict that just as we couldn't exactly predict when Go would be solved or something like that. Um, and then I think it's going to have lots of really cool applied applications. So um one of the things you want to do is you want to have provably stable, secure, unhackable, etc. software. So you can write math proofs about software and say this code not only does it pass these unit tests, but I can mathematically prove that there's no way to hack it in these ways or no way to mess with the memory or these type of things that hackers use um or has these properties. it can use the same lean and same proof to do formally verified software. I think that's going to be a really powerful piece of cyber security. Um, that's relevant for all sorts of other AI hacking the world stuff. And that yeah, if you can prove a rem hypothesis, you're also going to be able to to prove insanely complex things about very complex software. And then you'll be able to ask the LLM synthesize me a software that is uh I can prove is correct. Right? Why why hasn't provable um programming language taken off as a result of LLM? You would think that this &gt;&gt; I think it's starting to yeah I think it's starting to I think that one one challenge and we are actually incubating a potential focus research organization on this is the specification problem. So mathematicians kind of know what interesting theorems they want to formalize. Um if I have like some code let's say I have some code that like is involved in running the power grid or something and it has some security properties. Well, what is the formal spec of those properties? The power grid engineers just made this thing, but they don't necessarily know how to lift the formal spec from that. &gt;&gt; And it's not necessarily easy to come up with the spec that is the spec that you want for your code. People aren't used to coming up with formal specs and there not a lot of tools for it. &gt;&gt; So, you also have like this kind of user interface plus AI problem of like what security spec should I be specifying? Is this the spec that I wanted? So, there's a spec problem. Um and it's just been really complex and hard but but it's only just in the last very short time that that uh the LLMs are able to generate uh you know verifiable proofs of you know things that are useful to mathematicians um starting to be able to do some amount of that for for software verification hardware verification but I think if you project the trends over the next couple years it's possible that it just flips the tide that formal methods basically this whole field of formal methods or formal verification, provable software. &gt;&gt; Um, which is kind of this weird almost like backwater of more like theoretical part of programming languages and stuff. &gt;&gt; Um, very academically flavored often. Although there was like this DARPA program that made like a provably secure like quadcopter, helicopter and stuff like that. &gt;&gt; So secure against like what is the property that is exactly brewed? um &gt;&gt; and not for that particular project, but just in general like what because obviously the things malfunction for all kinds of reasons. &gt;&gt; You could say that um what's going on in this part of the memory over here which is supposed to be the part the user can access can't in any way affect what's going on in the memory over here or something like that or &gt;&gt; Yeah. things like that. Yeah. &gt;&gt; Got it. &gt;&gt; Yeah. &gt;&gt; Um so there's there's two questions. One is how useful is this? &gt;&gt; Yeah. &gt;&gt; And two is like how satisfying a as a as a mathematician would it be. Um and the fact that there's this application towards proving that software has certain properties or hardware certain properties obvious like if that works that would obviously be very useful &gt;&gt; but from a pure like are we going to figure out mathematics &gt;&gt; right? Um yeah. Is there is there sense that there's something about finding that one construction cross maps to another construction in a different domain or finding that oh this like lema is if you reconfigure it like if you redefine this um this term it still like kind of satisfies what I meant by this term but it it no long a counter example that previously knocked it down no longer applies like that kind of dialectical thing that happens in mathematics. Will the software like replace that? &gt;&gt; Yeah. I know like how much of the value of this sort of pure mathematics just comes from &gt;&gt; actually just coming up with entirely new ways of thinking about a problem. &gt;&gt; Yeah. &gt;&gt; Like mapping it to a totally different representation and yeah do we have examples of &gt;&gt; I don't know I think of it I think of it maybe a little bit like the when everybody had to write assembly code or something like that &gt;&gt; just like the amount of fun like cool startups that got created was like a lot less or something, right? And so it was just like less people could do it. Progress was more grinding and slow and lonely and so on. Um you had more false failures because you didn't get something about the assembly code right rather than the essential thing of like was your concept right. Um &gt;&gt; harder to collaborate and stuff like that. And so I think it will like be really good. Um there is some worry that by not learning to do the mechanical parts of the proofs that you fail to generate the intuitions that inform the more conceptual parts creative part right &gt;&gt; yeah the same with assembly and &gt;&gt; right and and so so at what point is that applying is vibe coding are people not learning computer science right or actually are they like vibe coding and they're also simultaneously looking at at at the LM is like explaining them these abstract computer science concepts and it's all just like all happening faster their feedback loop is faster and they're learning way more abstract computer science and algorithm stuff because their vibe coding, you know, I don't know. Um, it's not obvious that might be something the user interface and the human infrastructure around it. Um, but I guess there's some worry that people don't learn the the mechanics and therefore don't build like the grounded intuitions or something. But my hunch is it's like super positive. Exactly on net how useful that will be or how much overall math like breakthroughs or like math breakthroughs even that we care about will happen. I don't know. I mean, one other thing that I think is cool is is actually the accessibility question. It's like, okay, that sounds a little bit corny. Okay, yeah, more people can do math, but but who cares? But I think there's actually lots of people that like could have interesting ideas like maybe the quantum theory of gravity or something. like yeah one of us will come up with a quantum theory of gravity instead of like a card carrying physicist in the same way that Steve Burns is like reading the neuroscience literature and he's like hasn't been in the neuroscience lab that much but he's like able to synthesize across the neur neuroscience literature be oh learning subsystem steering subsystem does this all make sense he's you know it's kind of like he's an outsider neuroscientist in some ways um can you have outsider you know string theorists or something because the math is just done for them by the computer and does that lead to more innovation in string theory, right? Maybe. Yes. &gt;&gt; Interesting. &gt;&gt; So, &gt;&gt; okay. So, if this approach works and you're right that LLMs are not the final paradigm &gt;&gt; and suppose it takes at least 10 years to be the final paradigm. &gt;&gt; Yeah. &gt;&gt; In that world, there's this fun sci-fi premise where um you have turns to today had a tweet where he's like these these models are like automated cleverleness but not automated intelligence. And you can quibble with the definitions there. But &gt;&gt; yeah, if you have automated cleverness and you have some way of filtering, which &gt;&gt; if you can formalize and prove things that the LLM are saying you could do. &gt;&gt; Yes. &gt;&gt; Then you could have this situation where quantity has equality all of its own. &gt;&gt; Yes. And so what are the domains of the world which could be put in this provable symbolic representation? And furthermore, okay, so in the world where just AGI is super far away, maybe it makes sense to like literally turn everything the LLMs ever do &gt;&gt; or almost everything they do &gt;&gt; into like super provable statements. And so LLMs can actually build on top of each other because everything they do is like super provable. &gt;&gt; Yeah, &gt;&gt; maybe maybe this is like just necessary because you have billions of intelligences running around. Even if they are super intelligent, &gt;&gt; the only way the future AGI civilizations can collaborate with each other is if they they can prove each step &gt;&gt; and they're just like brute force turning out. &gt;&gt; This is what the Jupitter brains are doing. &gt;&gt; It's a univers it's a universal language. It's provable and it's also provable from like a are you trying to exploit me or is are you sending me some some message that's actually trying to like sort of hack into my my brain effectively? Are you trying to socially influence me? Are you actually just like sending me just the information that I need and know more right for this? And yeah, so Davididad who's like this program uh director at Arya now um in the UK. I mean he has this whole design of a of a kind of uh ARPA style program a sort of safeguarded AI that very heavily leverages like provable safety properties and um can you apply proofs to like can you have a world model but that world model is actually not specified just in neuron activations but it's specified in you know equations. Those might be very complex equations, but if you can just get insanely good at just autoproving these things with cleverness, auto cleverness. Can you have, you know, explicitly interpretable world models, you know, um, as opposed to neural net world models and like move back basically to symbolic methods just because you can you can just have insane amount of ability to prove things. Yeah, I mean that's an interesting vision. I don't know how, you know, in the next 10 years like whether that will be the vision that plays out, but I think it's really interesting. um to think about. Yeah. And even for math, I mean, I think Ta Terry Tao is like doing some amount of stuff where it's like it's not about whether you can prove the individual theorems. It's like let's prove all the theorems on mass and then let's like study the properties of like the aggregate set of proved theorems, right? Which are the ones that got proved and which are the ones that didn't. &gt;&gt; Okay. Well, that's like the landscape of all the theorems instead of one theorem at a time, right? &gt;&gt; Speaking of symbolic representations, one question I was meaning to ask you is how does the brain represent the world model? like obviously nets out in neurons, but I I don't mean sort of extremely functionally. I mean sort of conceptually is it in something that's analogous to the hidden state of a neural network or is it something that's closer to a symbolic language? &gt;&gt; We don't know. I mean I think there's there's some amount of study of this. I mean there's there's these things like you know face patch neurons that represent certain parts of the face that geometrically combine in interesting ways. That's sort of with geometry and vision. Is that true for like other more abstract things? There's like this idea of cognitive maps. Like a lot of the stuff that a rodent hippoc campus has to learn is like place cells and like where is the rodent going to go next and is it going to get a reward there? Um is like very geometric and like do we organize concepts with like a a spa abstract version of a spatial map. Um there's some questions of can we do like true symbolic operations like can I have like a register in my brain that copies a variable to the another register regardless of what the content of that that variable is. That's like this variable binding problem. &gt;&gt; And basically I just don't I don't know if we have that like machinery or if we or if it's like more like cost functions and architectures that like make some of that approximately emerge but maybe would also emerge in a neural net. There's a bunch of interesting neuroscience research trying to study this what what the representations look like. What was your hunch? &gt;&gt; Yeah, &gt;&gt; my hunch is it's going to be a huge mess and we should look at the architecture as the loss functions and the learning rules and we shouldn't really I don't expect it to be pretty in there. Yeah, &gt;&gt; which is it is not a symbolic language type thing. &gt;&gt; Yeah, probably probably it's not that symbolic. Yeah, but but but other people think very differently, you know. Yeah. &gt;&gt; Another random question. Speaking of binding Yeah. What what is up with feeling like there's an experience that it's like both all the parts of your brain which are modeling very different things have different drives feel like at least presumably feel like there's an experience happening right now and also that &gt;&gt; I don't know &gt;&gt; across time you feel like what is uh &gt;&gt; yeah I'm pretty much at a loss on this one um &gt;&gt; I don't know I mean Max Hodak has been make giving talks about this recently he's another really hardcore neuroscience person um neurochnology person um and the thing I mentioned with Dorso um maybe also it sounds like it might have some touching on this question but uh yeah I think this I have I don't think anybody has any idea [laughter] it might even involve new physics it's like you know yeah &gt;&gt; uh another question which might not have an answer yet what um so continual learning &gt;&gt; is that the product of something extremely fundamental the level of even the learning algorithm where you could say look at least the way we do back propagate neural networks is that you freeze the way there's a training period and you freeze the weights &gt;&gt; um and so you just need this active inference or some other learning rule uh in order to do learning or do you think it's more a matter of &gt;&gt; architecture and how is memory exactly stored and is it like what kind of associative memory you have basically &gt;&gt; yeah so continual learning Um, [gasps and snorts] I don't know. I think that there's probably things that there's probably some at the architectural level, there's probably something interesting stuff that the hippocampus is doing. Um, and people have long thought this. Um, what kinds of sequences is it storing? How is it organizing, representing that? How is it replaying it back? What is it replaying back? um how is it exactly how that memory consolidation works in sort of training the cortex using replays or or or or memories from the hippoc campus or something like that. Um there's probably some of that stuff. There might be multiple time scales of plasticity or sort of clever learning rules um that can kind I don't know can sort of simultaneously kind of be storing sort of short-term information and also doing back prop with it. and neurons may be doing a couple thing, you know, some fast weight plasticity and some slower plasticity at the same time or synapses that have many states. I mean, I don't know. I mean, I think that from a neuroscience perspective, I'm not sure that I've seen something that's super clear on what continual learning what causes it, except maybe to say that this this systems consolidation idea of sort of hypocampus consolidating cortex like some people think is a big piece of this and we don't still fully understand the details. Yeah. Speaking of fast weights, is there something in the brain which is the equivalent of this distinction between parameters and activations that we see in neural networks? And specifically like in transformers, we have this uh idea like some of the activations are the key and value um vectors of previous tokens uh that you you build up over time. And they there's like the so-called the fast weights that you whenever you have a new token you you query them against these um you query these activations but you also obviously query them against &gt;&gt; all the other parameters in the network which are part of the actual built-in weights. Is there some such distinction that's analogous? &gt;&gt; I don't know. I mean we definitely have weights and activations. whether you can use the activations in these clever ways. Um, different forms of like actual attention, like attention in the brain. Um, is that based on I'm trying to pay attention. I think there's several probably several different kinds of like actual attention in the brain. I want to pay attention to this area of visual cortex. I want to pay attention to this &gt;&gt; the content in other areas that is triggered by the content in this area. Yeah. Right. &gt;&gt; Attention that's just based on kind of reflexes and stuff like that. So I don't know. I mean I think that there's not just the cortex. There's also the phalamus. The phalamus is also involved in kind of somehow relaying or gating informations. There's cortical cortical connections. There's also some amount of connection between cortical areas that goes through the phalamus. Is it possible that this is doing some sort of matching or kind [snorts] of uh uh uh constraint satisfaction or matching across you know keys and you know keys over here and you know values over there. Is it possible that they can do stuff like that? Maybe. I don't know. Oh, this is all part of what's the architecture of this corticothalamic yeah system. Um I know I don't know how transformer-l like it is or &gt;&gt; if there's anything analogous to like that attention be interesting to find out. [laughter] &gt;&gt; We got to give you a billion dollars so we can you come on the podcast again and then tell me uh how exactly the &gt;&gt; work mostly I just do data collection just like really really unbiased data collection so all the all the other people can figure out these questions. Yeah. Maybe the final question to go off on um is what was the most interesting thing you learned from the gap map and maybe you want to explain what the gap map is. &gt;&gt; So the gap map so in the process of incubating and coming up with these focused research organizations these sort of nonprofit startup like uh moonshots um that we've been getting philanthropists and now government agencies to fund. Um, we talked to a lot of scientists and some of the scientists were just like, "Here's the next thing my graduate student will do. Here's what I find interesting." Exploring these really interesting hypothesis spaces like all the types of things we've been talking about. And some of them are like, "Here's this gap. Um, I need this piece of infrastructure which like there's no combination of like grad students in my lab or me loosely collaborating with other labs with traditional grants that could ever get me that. I need to like have like an organized engineering team that like builds, you know, the the mini miniature equivalent of the Hubble Space Telescope. And if I can build a Hubble Space Telescope, then like I will unblock all the other researchers in my field or or some like path of technological progress in the way that the Hubble Space Telescope made lifted the boats, improved the life of every astronomer, but wasn't really an astronomy discovery in itself. It was just like you had to put this giant mirror in space with a CCD camera and like organize all the people and engineering and stuff to do that. Um, so some of the things we talk to scientists about look like that. And so the gap map is basically just like a list of a lot of those things and it's like we call it a gap map. Um, it's I think it's actually more like a fundamental capabilities map like what are all these things like mini hub space telescopes. Um, and then we kind of organize that into gaps for like helping people understand that or like search that. &gt;&gt; And what was the most surprising thing you found? So I mean I think I think I've talked about this before but I think it one thing is just like kind of like the overall size or shape of it or something like that is like it's like a few hundred fundamental capabilities. So if each of these was like a deep tech startup size project that's like only a few billion dollars or something like you know each one of those was a series A that's only like not you know it's not like a trillion dollars to solve these gaps. It's like lower than that. And so that's that's like one maybe we assumed that and we also came to that's what we got. It's not really comprehensive. It's really just a way of summarizing a lot of conversations we've had with scientists. Um I do think that in the aggregate process like things like lean are actually like surprising because I did start from sort of neuroscience and biology and it was like very obvious that there sort of like these omix we need genomics but we also need contoics and you know we can engineer E.coli but we also need to engineer the other cells and like there's like somewhat obvious parts of biological infrastructure. I did not realize that like math proving infrastructure like was a thing. And so &gt;&gt; um and that was kind of like emerging from trying to do this. So I'm looking forward to seeing are there other things where it's like not actually this like hard intellectual problem to solve it. Um it's maybe the kind of slightly the equivalent of AI researchers just needed GPUs or something like that and focus and and and really good pietorch code to like start doing this. Like what is the full diversity of fields in which that exists? Um, we've even now found and which are the fields that do or don't need that. So, fields that have had gazillions of dollars of investment, do they still need some of those? Do they still have some of those gaps or is it only like more like like neglected fields? Um, we're even finding some interesting ones in actual astronomy, actual telescopes that have not been explored because maybe because of the kind of &gt;&gt; um if you're getting above a critical mass size project, then you have to have like a really big project and that's a more bureaucratic process with the federal agencies. Yeah, &gt;&gt; I guess I guess you just kind of need scale in every single domain of science these days. &gt;&gt; Yeah, I think you need scale in many of the domains of science and that does not mean that the lowcale work is not important. uh does not mean that kind of creativity, serendipity, etc. um each student pursuing a totally different direction or thesis that you see in universities is not like also really key. &gt;&gt; But yeah, I think we need some amount of scalable infrastructure is missing in essentially every area of science, even math, which is crazy because math mathematicians I thought just needed whiteboards, &gt;&gt; right? &gt;&gt; Right. But they actually need lean. They actually need verifiable programming languages and stuff like like I didn't know that. Right. &gt;&gt; Um cool. This is super fun. Thanks for coming on. Thank you so much. Where can people find your stuff? My pleasure. &gt;&gt; Uh the easiest way now, my my adamarbles.org website is currently down, I guess, but [laughter] you can find uh convergentresarch.org can can link to a lot of the stuff we've been doing. Yeah. &gt;&gt; And then you have a great blog, longitudinal science. &gt;&gt; Yes. Longitudinal science. Yes. On WordPress. Yeah. &gt;&gt; Cool. &gt;&gt; Thank you so much. Pleasure. Yeah. &gt;&gt; Hey everybody, I hope you enjoyed that episode. If you did, the most helpful thing you can do is just share it with other people who you think might enjoy it. It's also helpful if you leave a rating or a comment on whatever platform you're listening on. If you're interested in sponsoring the podcast, you can reach out at dwarcash.com/advertise. Otherwise, I'll see you on the next one.